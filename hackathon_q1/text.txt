Hey, Alex, good to see you. Hey, you too. How's it going? Doing well. Doing well. I'm in office today. First time in three months. I'm feeling. It looks like you're at your house. Yep. Very nice. I traveled last week, but we're back now. Okay. Very nice. Yeah, I traveled this morning. Alex, remind me, where are you located again? I am in Grand Rapids, Michigan. Okay. Nice. Cool. All right. Looks like we got David on. David, welcome. Nice to meet you. Nice to meet you. And we have some new faces on our side, so maybe we can start with introductions today. The agenda is pretty straightforward today. Really want to dive into RL, the training side of things as well, even show you guys a demo. And on the call I have today, I have Uday. Uday is one of our lead sales engineers who will perform a demo today on the AnyScale platform, as well as Karosh, who leads our RL team and helps innovate on the package of RL live, which you guys aren't using today, but maybe it's a possibility down the line. And we want to dive into that. And it looks like, David, you posted some questions via email. We saw those and we will be able to answer all those via this discussion. So maybe a quick introductions on your side, Alex, you want to give a quick intro? I know you and I met already, but just for the rest of the team here. Yeah, sure. So I'm Alex. I've been at Slingshot for about six months now. I'm a machine learning engineer. And I typically focus on my background is actually in more along the lines of software development than is machine learning, though I do have a master's in machine learning or in data science. But so I've been focused mostly on the software side of sort of focusing on the deployment side really of how do we take the take the machine learning models and apply them within our within our applications. So awesome. Yeah. And David, over to you. Yeah. So I'm Dave. Can you hear me all? Yeah. Loud and clear. Yeah. So like Alex, I joined Slingshot about seven months ago or something. And yeah, prior to Slingshot, a lot of my work was in ML, but I was focused on the problems from scientific machine learning to image detection and classification. I did a little bit of reinforcement learning a few years ago, but kind of since I've come to Slingshot, that's been my main area of focus and we've been really trying to stand up some capability to answer some questions, questions for sequential decision making. So yeah, I've I've had some experience like looking at Ray, but not don't, you know, didn't dive in. No, that's good. Appreciate that. Well, awesome. I think the agenda today, Alex, again, I think we want to show you guys a demo, but I think we were quick want to recap our last conversation you I had a lot of some questions, maybe go into Ray in our live even further and then go through that demo. Alex, that OK with you on your side? Great. OK, cool. So just to summarize our last conversations very quickly, it was semi brief, but it was super impactful at the same time. It sounds like you guys are looking to potentially scale, right? You have seven models in production today. It sounds like it looks like you guys are going to hit that scale period where you need infrastructure to back you up, whether that's, you know, training in an efficient manner and streamlining that all the way to serving as well as R.L. And it sounds like you're looking for a platform, at least in the you know, you're exploring platforms out there to start and you look for something maybe can handle R.L. and training and tuning and serving all on one unified kind of frameworks that slash platform. So that's what we offer today. And we want to get into, you know, what Ray is again, what R.L. live offers over something like a stable baseline. David, maybe what you're using today and then show any scale, how any scale differs from the open source project we're supporting. So, David, just for your background, you know, Ray is open sourced, so it is free to use, free to download. You can kind of deploy how you'd like any skills, the managed version of Ray out of the box on any of us to scale it up. So let me pause there. Alex, do you want to throw any additional comments in there? Anything I got wrong? We can kind of begin the conversation from here. No, that sounds good. As we I guess I would add in, I'm curious, just so well, no, I'll I'll save my questions for the end. How about that? That's in particular. But we'll we'll we'll discuss afterwards after that. Cool. So, David, just to clarify, are you focused on R.L.? Are you focused on like the whole everything ML deep learning R.L.? I guess we're so I've experienced across the board, to be honest. But my main tasking, it's like, OK, OK. Is there a separate key focus on the ML side or just it's just kind of you two manning that data science team, like leveraging the PyTorch packages? I think you mentioned TensorFlow. I think you guys are using that. Yeah, we have a we have a few other data scientists. I didn't want to pull them all them all. But David, sort of the reinforcement learning lead. And then other than that, it's sort of the traditional a little bit more of the traditional machine learning side of like neural networks, PyTorch, so I can learn that kind of stuff. Gotcha. OK. And it sounds like David, you're performing R.L., but nothing's deployed just yet. So it sounds like you're still in the research phase and building phase, which is great. Cool. Karash, any questions? I know we should probably go into R.A. and R.L. a little further, but any questions for clarification on your guys' side? Yeah. Before we dive into it, like we can for the R.L. part, I think the questions are like very interesting. We can dive deeper into them. But I before getting into that, I would like to know what what do you think the value of R.L. would be to your company? Like what are you trying to achieve on a high level maybe? Is it simulator based offline or like what is the what are you looking for? Yeah. So we have so to be model based R.L. We have some very in-depth high fidelity simulation capabilities that we've been developing and are currently developing as well. And so from the reinforcement learning kind of perspective, and I'll say reinforcement learning, but really what we're after is really sequential decision making and figuring out, you know, what are the best decisions that we can make to help our customers and our customers are wide ranging. So there's at a high level, there's like quite a few different problems that we'd like to solve for. Like, how do you make the best maneuver in situation X in an orbital sense or something like that? And we think reinforcement learning can answer that question, but we also might not be additional approaches out there. Yeah. So does that answer your question? Well, I think you might be muted. Yeah. Yeah. We're all in the same room where we couldn't get the visualization going. So basically like the problem is not like a design problem. It's basically the traditional RL where you need to learn how to act in like a simulator. Okay. Got it. Cool. So should we go on the question one by one? Yeah. We could start with those questions, David or Alex and Dave, does it make sense to kind of give an high level overview of Ray to kind of set the foundation and go to our, what do you think about this Alex? We can leave this demo first demo first and we can kind of jump in as we have questions, but and then we can get to the RL specific stuff after that. Okay. Who would like to do the demo? Okay. Who would like to do the demo? Okay. You just need the second to get started here. Okay. Can you guys see my screen? Yeah. Okay. Perfect. So this is basically the AnyScale platform. So what you see here is our main landing page. To give you a sense of, if you played around with Ray, Ray is a distributed framework to build ML applications and AnyScale is the managed solution to run your Ray applications faster, cheaper, and with higher reliability. So on the left, you can see the different sort of features that we have provided. So at the top most here we have, so the way to think about it is like AnyScale, we want to accelerate our journey from development to production. So for, to accelerate development, we have this concept of workspaces, which is essentially like a development environment that runs on the cloud. So the idea is like, typically you guys must be very familiar with this, right? Like when you're developing ML applications, you focus on the application, you try to make it work on one machine. And then when you want to scale it out to several machines, that's when sort of, there's like a big lift required to translate that application code and make it more distributed. So with workspaces, what we've tried to do is solve that by essentially blending, you know, your development and your remote workbench. So I'll dive deeper into this in a bit, but you know, like workspaces is one way. The other way you can develop with Ray is using interactive sessions. So here, you know, using our AnyScale product, you can spin up, you know, one or many clusters and then using your favorite IDE, you can like launch jobs against those clusters. And you know, this section, this tab in the product lets you monitor and look at the different jobs that you run, you know, against your cluster. You can also inspect the logs. You can, you know, look at how much each of these development jobs are costing, how much time they took, so on and so forth. Now. Yeah. Sorry. One question about this. So these are, you're calling these clusters, are they, are they actually clusters? Are they like, because I assume they're not like, they're not like individual Kubernetes clusters, right? I'm just trying to get the terminology right in my head. Yeah, they are. So these are actually EC2 instances running in your cloud account. Yeah. Okay. And then sorry, for the workspaces, you showed us a terminal, but it looks like there's a Jupyter logo up above. Could you, could you do a notebook if you wanted? Yeah, absolutely. So let's just jump into this then. So so yeah, I mean, basically, you can launch a Jupyter session or a VS code session, right? So we support both those. So in this case, you can launch a new terminal and then or sorry, a Jupyter session and then anything you do here is going to actually run on this remote cluster. Okay. Cool. Okay. And then you can Yeah. And so so that's one way to interact with the workspace. The other way is you can actually launch, you know, visual session on your local Visual Studio code. So if you're used to developing using the desktop client, so like, for example, I can create a new file here, let me call it like test.py. You know, and then I write a short script, like, right, and then I run this. So now if I go back to, you know, the terminal on the remote session or on the remote cloud, I'll see this test.py, right? So it's like, as you're developing locally, like it's actually syncing your, you know, your files to the remote cluster. And so, you know, you can actually develop like a developing on the cloud, cool. And then, you know, like, while you know, when you're creating a workspace, there are a lot of things, you know, ways you can configure that workspace. So at a high level, the three different ways are, you can define something what we call is a cluster environment. So these are basically a set of dependencies that will be installed on every node in your cluster, right? So we, you know, out of the box, we have several cluster environments that are suited for, you know, machine learning applications, or, you know, not, you know, regular like application, different Python versions, things like that. Or if you know, there's something which doesn't meet your requirements, you can actually navigate to the configurations tab, and then just, you know, create your own cluster environment. So either you can install, you know, various packages, Conda packages, define environment variables, and additionally, you know, you can also enter, you know, ad hoc postal command to set up that environment for your application. Lastly, if you already have a CI CD pipeline in place, you know, you can, what you can do is just like point, you know, provide, enter like a URL for an artifact repository and like just pull that image on and prove it and, and any scale will provision it to, you know, every node on the cluster. Awesome. Hey, Alex, remind me, how are you guys developing today? It's just on your laptop. Yeah. I mean, pretty much locally, or with just kind of like ad hoc EC2 instances. Okay. Dave, does that capture kind of what you're doing? Yeah. For the most part, we've run on single instances for the RL one, just spin up some large machines and just distribute processes. Or also we use, we use SageMaker sometimes for the more traditional stuff. And then how does it work? David builds something, pass it to you, Alex, can you put it in production, how the handoff works? I mean, in a sense, it's a little bit, well, we haven't done that yet with, with Dave. I mean, the, really the objective here, right, is, is to minimize that handoff as much as possible. And to really have the data scientists have the ability to self-serve their models all the way through. Right. So like for now, I do a lot of work, but typically with the data scientists, it's not like a, like a hard handoff. Gotcha. But, you know, the, the real objective here, and one of the reasons why we're looking at frameworks like this is to, you know, do my job. Right. Well, no, I guess help you with your job. Help me with my job. Let me do your jobs, right. But yeah, I see your point. Yeah. Just, you know, in a perfect world, the data scientist is responsible for the model. Yeah. And so they should be responsible for as, as much of the model life cycle as is possible. Yeah, absolutely. Yeah. I mean, this essentially would handle the infrastructure for both your training, RL, and serving if that's the direction you want to go, right. So you know what, we work with a larger company called West Farmers out of Australia. You might know them, maybe not, but they would build a model and then have to get that in production. They're going to have to rewrite the code 7% of the time and then production, you know, with any skill, it's, it's a unified platform for developing and production. And just one couple lines of code change pushes it in production for the data science themselves. So I could see how that pain point can be solved here with that. Yeah. Can we, can we talk about the, the serving aspect of this? Yeah, let's, let's continue down the development side and we'll get right into the serving side. Sure. I have a question for David, you mentioned you use a single instance, a machine for your scaling applications. How big is your, like, how, how big is the requirement? Do you have any pain points about like availability of those instances at all? Like, well, so we're just running on a single machine. So a single EC2 instance, and then we just distribute our simulations to, you know, multi code, but your use case, haven't gone beyond like the resources on a single. We want them to, like we're, so the simulations that we run are not incredibly computationally costly, but they're costly enough. And so it makes training slow. Like if we want to train some of the models, our own models, just on like the single instance that we, large instance that we have, it can take place, which I'd like to speed that up, ideally. Yeah. Something that I think was not, I don't know if it's going to be mentioned on the product is like the auto scaling capability where you start developing on a small instance machine, like let's say M5, right? That had 32 CPU's, something like that. You develop your thing and then when you want to scale it up, you just like change a parameter number of workers, let's say a thousand. And then it just suddenly gets like the same code runs on the same cluster. And then you get pulled in new instances attached to your head and then you jump, just run your like job at more scale. That saves tons of money when you don't want to develop on like the bigger, bigger instances for development and then just want to do it at, then the experiment is ready. Yeah. That sounds good to me. Yeah. We're actually going to that right now. So good time. The way to configure it is, you know, right here. So as Karush was saying, you can define like different instance types for your head nodes. But additionally, depending on your workload, you can define heterogeneous clusters. You know, where, you know, you can define like pools of worker nodes to which you can, you know, deploy like certain parts of your application. And so here is where you can specify, you know, how many nodes a particular worker pool can auto scale to. And additionally, if you're very cost sensitive, you can also, we also do support spot instances that can significantly reduce, you know, the cost of these workloads. So the advantage there is that even though you use spot instances, like, you know, any scale guarantees like a certain level of fault tolerance. So if you're running, you know, experiments over days or weeks, like, you know, we have like all the like features built into like pickup from a checkpoint whenever a node goes down. Lastly, you know, if there's like, if you if, you know, you need to tag your instances in a certain way, we do provide, you know, the ability to do that as well. You know, in this instance, config section. And then lastly, we have certain time and activity based, you know, features, the cost saving features. So here you can specify like a certain period of time after which the cluster should terminate if there is no activity. Are you guys using spot today? Or is that something you guys would be interested in? We're not using it today. It could be. I mean, it would depend on the use case, right? But no, we're not using it today, I guess. And then Alex, are you finding on your side since you're kind of more on the upside, like data scientists, or maybe David himself leaving collectors up and running when using EC2? Or is that a feature of turning it off to ensure with no wasted costs important to you? Or they would say, well, it is right. We we are not the biggest offenders at the moment. Okay. Yes. Right. As we grow, it's definitely something that we need to be conscious of. Right. Okay. But yeah, it hasn't been something that is terribly horrible right now. But obviously, it doesn't it doesn't take much to just forget about something and suddenly it's $10,000. A dagger, yeah, now I hear you. Okay, awesome. Over to you Uday. Okay. So I wanted to also quickly show you guys, so we do have some observability features also built in. So there are several CLI commands that are well documented, where you can inspect like, you know, the health of the cluster and things like that, like, but then also, you know, there's like, several dashboards where you can look at different characteristics, you can look at the, you know, state of the scheduler. So there is schedulers constantly, you know, dispatching tasks and actors are currently don't have anything running, which is why most of them are empty. But we can imagine like, this would be well visualized. Additionally, we also capture cluster metric, like CPU, GPU utilization, compute utilization, you know, memory, disk usage, things like that. So you know, using these, you can easily profile and even debug your application. Additionally, if there's like, you know, something that isn't available out of the box, we also have Grafana, so you can, you know, build your own dashboards and yeah, like, you know, so that sort of covers the development journey now to go to Sorry, can I ask one other thing, and this maybe is a larger thing. But while we're talking about the development side, one of the things that we care about is things like experiment tracking. Is that something that you support out of the box? Or is there is it something that we could potentially integrate other tools? Yeah, yeah, great question. So we, like, we, we don't have anything out of the box, but we have, you know, integrations with weights and biases and, you know, potentially, yeah, you know, you can you can tenderboard like that. You can set it up yourself. Yeah. Okay. Gotcha. Yeah. The reason for that, Alex, is we're looking to own the computer, right? Right. We're not looking to build that an average kind of tracking monitoring, right. And the biggest out there ready, which is weights and biases. So yeah, I think I think that makes a lot of sense. But I want to make sure or I want to understand how we can, how we would be able to to still get those aspects right. While you while utilizing something like your, like your workspaces, how does that integrate into something where we can kind of track our, our data sets and experiments and that kind of thing? Yeah. I mean, so we do have, I mean, what do you use for experiment tracking right now? Well, we don't we don't yet. Not yet. But it's definitely it's something that we're we're keeping in mind, like as we as we eventually decide on on what to what path forward we take, we that's one of the things that we're going to need. Yeah, we actually just released a partnership with weights and biases first party. There's actually a webinar that just came out, I think last week, but I think that's what you're showing today. Yeah, yeah, that's what I was pointing to, like, you know, in the documentation, there's like a really good example of how you could, you know, integrate rates and biases. So in this case, for example, like, you know, in the training function, you can define callbacks, which basically, you know, pipe out like these, you know, the training metrics to like weights and biases, and then they sort of, you know, visualize and, you know, you can sort of use like their UI to sort of, you know, monitor your experiment. So we have a couple of customers using this. Another example on how you can integrate with ml flow. Okay. So if there is, like, if you decide to use something which isn't here, we'd be happy to like support that. Okay. Cool. So coming to the production side of things now. So you know, so like, let's say, you know, use work faces, you're, you know, iterating, you get your workload out on like, you know, large clusters, and now you want to, you know, productionize this, we offer like a couple of ways to do that. The first is what we call a production job. So you know, you can think of this like a cron job, which can be scheduled to run, you know, once, you know, like, depending upon a particular schedule. And what endscaling will do is it will manage the orchestration of spinning up a cluster, you know, running your Python script on it, scaling it out. And then once the job is complete, like, you know, carrying the cluster down. So this is used by a lot of our customers, you know, for periodic jobs, or, you know, certain experiments that they want to be, you know, running once every certain period of time. So for example, like, in supply chain, like, you know, you would want to, since your SKUs are, you know, they come in, like, at a, like, you know, very, you know, slow speed, right. So we have customers that are retraining their, you know, algorithm, like on a schedule, like at the beginning of every month. And they, so they run these jobs, you know, and it gives them several different, you know, models and then they, you know, check performance and then deploy it in their production workflows. Do you guys see that as a punchy use case for you guys, or is that something more, or if at all? So you're talking sort of retraining, or we certainly have the idea of, like, maybe a scheduled or sort of batch use case. Is it fair to say, you don't have that today, but potentially on the line, something you'd like to, okay, cool. Yeah, cool. So yeah, I mean, so that, you know, as you can probably, like, extrapolate it, it's very self-contained, right. So easily, if you're, especially for cost tracking, you can, you know, it's good to get a sense of, like, how much my training job is costing and, you know, like, any scale help to do that quickly. The second way to sort of productionize your app is using any scale services. So here you can, you know, stand up an endpoint, which is, like, you know, always on and available for, you know, your users to make requests. So again, what any scale will do is, like, it can, you know, scale out these deployments based on the number of requests that come in. And then, you know, we have, like, different components set up that ensure, you know, the high scalability, reliability, and performance. I'll maybe quickly touch upon how you can set up a production service since you asked about it. So essentially, so there, so at the application level, there are a few things you would have to do, and all of this, again, is, like, you know, available in our documentation, but let me just quickly touch that. And Alex, while you're doing that, you have, I think you mentioned the number seven, but I think that's the goal of maybe this year. I guess how many models do you have in production today, or is that something you're trying to get to? Today, I think there's one, and we expect that to grow a lot. The exact number is hard to say. Maybe the verb that you used was, if we got the seven or eight, we'd have a scaling problem. Yeah, maybe something like that. Okay, okay. And that one you're using today, is that via SageMaker, or is that? Yeah, it's your SageMaker. So yeah, here's a very simple example of how you can, you know, convert your application code into a race of deployment, and it's literally like just adding a decorator to the class. If you want to, you know, set up like different routes, you know, on functions, you know, you can do that. And all of this is detailed in the documentation. And this would be like the first step, basically, you would have to, you know, make a few changes to your application code. And then next, it would be to define like a YAML file in services, here, you'll define your entry point, you know, which has a Python, you know, your application, you'll have to define a cluster environment and a compute config. And then that's it. And in any scale, we'll like manage the rest. Additionally, we do support zero downtime upgrades. So you know, when you want to change, like upgrade, you know, the version of your application code, you know, we support that as well. And like, the way to do it is literally just upgrade your YAML file and then, you know, redeploy your service and then, you know, any stable ensure that, you know, all user requests are sort of serviced while it's making that change. Yeah, I think that covers most of the platform. Yeah. Alex, I'm curious the volume where the production model gets pinged, is it like, is it, you know, is it consistent? Or is it like peaks and valleys? Like, how does the flow looking like and are you auto scaling with with the demand? At this point, we haven't really had any issues. The volume is low right now. A lot of them are running batch, though. So but yeah, I want to make sure that we have some time to talk about the the RL stuff. Yeah, let's do that. That's we have about 10 minutes left. I think we can go a couple minutes over. We if you guys are good with that, but if not, we can I think we have 10 minutes left of content. I'll pass you over to Karosh to take that away. Should I go over the questions first one by one and then deeper if you need to? I think I think maybe a quick like overview verbally, maybe just like why RLib was built and the value of overstable baseline and other tools and maybe answer those questions. So RLib was this library that basically was built on top of Ray to serve, you know, RL components. And something that is interesting about RL is like in the early days, I think it's still true is like the scaling of environment sampling was a problem. And Ray was just like great for that. You just like spawn out different environments and like just scale that horizontally and you just accelerate your sampling. And if your environment sampling is a bottleneck, RLib was the original solution to that. I think that's something that no other library actually has, like neither stable baseline or any other library. We also have a scaling story for our model training. So if you want to do multi GPU training, you know, maybe you want to do some sense, some sort of like decision tree, you know, models for your policies and you want to scale them up across multi CPUs, RLib does that for you. It takes care of that, you know, on the training stack as well. So the kind of like most important value proposition of RLib over other libraries is like scale availability and it's ready to get a scale if you want it to. So there's that. It has a lot of like algorithms that it supports out of the box. We have like on policy algorithms, off policy algorithms, like APEX family, which is like asynchronous algorithms, which are not possible to do in like traditional, like, you know, stable baseline, which doesn't support this like notion of workers that are asynced with each other. So and then like distributed replay buffers for just scaling up offline algorithms, sorry, off policy algorithms. So that's kind of like the benefits that come with the scalability part support of like all these different algorithms. We have support of multi-agent RL out of the box and offline RL, which is something that no other library kind of, you know, packages under the same group. So if you want to, for example, pre-train a model using offline data and then do online sampling, that's something you can do easily with RLib. Whereas in other libraries, you have to do like, you know, mixed solutions. You have to pick out like this thing from a stable baseline and then integrate it with something like that. Those offline are like pre-agent, right? And something else is like that we do is like both framework supports like Torch and TensorFlow based on your production. Torch is catching up with the speed, like with their 2.0 version, like Python, Torch.compile. But I think before that, TF was still ahead in terms of performance. So some people like for the ease of development go with PyTorch, but for production readiness, I think like TF was the go-to solution. Yeah. So I think like, and also like something you mentioned in your questions was the external environment support, where you have like the GCPs. Over the years, I think we have made like different modifications to increase the support for these different environment workloads. So you can have like external environments, client server kind of environments and vectorized environments like, and vectorize things on top of the existing environment solutions that you have. So that's basically what I basically call like better environment support, where like for other things, you may want to create custom things that like, you know, make it like a, make the handshake to those existing libraries possible, like, you know, possible. You have to do a lot of coding to make your environment compatible with those existing environments. Whereas like in our lib, I think it mostly supports that right off the bat. Yeah. That's basically the whole, like a holistic view of the value proposition of our lib and why you think like, it's kind of like, we call it like industry grade RL solution as opposed to like a research solution. I think if you're in a very early stage exploration where you want everything to be visible, usually like a stable baseline is a better go-to, but like if you want to scale things up, you want fault tolerance, you want, you know, to connect if, you know, weird looking environments, then our lib becomes a better solution. So to that, to talk the follow-up to that, it makes sense with the scale, the one thing that I guess I originally attracted me about stable baseline is it is very kind of clear what it is doing. And we actually have our own fork and make changes to the underlying kind of there. How open is that on like the RL lib side? So the training stuff is pretty open. Like if you, I think like there is like different levels of complexity that was introduced because we wanted to support this scalability and also like these environment loops, right? So I think that training part is very transparent. The learning curve is like different. I think our lib is going to have a steeper learning curve and you're actually taking some initiatives to actually lower that barrier. We are redesigning our APIs in a way that are more intuitive based on the learnings we had from like comparing to these other libraries to make it more like research friendly as well. But that has not been exit, you know, landed on the RL lib that we have today, but the RL lib we have today has, I would say it has a good like semi-transparent training steps, but the environment sampling is a bit more involved and convoluted. So, but that's usually the part that people don't want to deal with. That's like implementation. You just put in your environment and you get your batches of samples and then you have customizability on the loss function, on how you want to do like the dynamics of your training, like replay buffer sampling, those kinds of things. And if this is something that you want to share, like to the public, it's open source. You can basically become like an open source user, make PRs, we review it. It goes through the CICD thing, like normal open source stuff, and then gets landed on the master version of the RL lib. We, a lot of people don't work like RL lib because down the line, we're going to introduce features that they wanted, but they cannot merge at some point. And they have like divers too far to kind of go back. Yeah. So that's, that's why it's kind of open source to kind of bring back the improvement. Yeah. Hey, David, I just have an extra like five minutes after the 45 mark or are you guys just, okay, cool, cool. So should we go to the questions to ask, like answer them specifically? Cool. So yeah, the first question was like, do we support environments using multiple processes that communicate or gRPC? Yes, we actually have like a customer called Riot Games. They're like in the gaming industry, they do this with like their gaming environments, they have to connect to these external environments that are living on a Windows machine, for example, right. And they have to like do that communication using this gRPC stuff. I have some pointers, I can reply to that email and then send you like how we do it. Like there are some unit tests that showcases like the use case, but there's also like end to end examples that may be good starting points. The next question was, I'm going to, by the way, send a like holistic answer like to that email. The next question was, do we support custom action or observation spaces? And you mentioned that for your action spaces, you need to extend the box, right? We don't support it the way you describe like, for example, if you just subclass gm.box and then do something with it. I think our lib won't support it out of the box because it makes some assumptions about whether it's a box through this neural network, right? And that those things will probably break. But what we have supported so far and has been sufficient. So I want to like, you know, basically get more context about your use case here is like we have extended the gm spaces to these like, you know, multi-discrete, some sort of like repeated actions. So like there are some extensions that we have provided already that has covered most of the use cases, all the use cases that our customers had so far. And we think that has been sufficient, but I'm more curious to know about like why you extended box action space, what are you trying to do, you know, essentially give a better answer to elaborate on that a little more. So within the box action space, the main point of that we were having problems with is the sample call because box assumes uniform in each direction. But we have this case where we're trying to sample a polar coordinate space or operating within a trig space. And so within that trig space, we actually have to sample over like zero to two pi, but zero is equal to two pi. So how we got around it was we created two parameters instead of just the one to enable uniform sampling over that. Got it, got it, got it. I have to think about it, that's an interesting, so basically you want to define like this, you know, sampling of the angle, right? That is two pi and then you want it to be uniform across that. So if you sample, so just to like for clarity, if you only, if you call it two dimensions, call it like theta and phi and you sample uniformly, you end up with a sampling bias of the poles. Additionally, if you sample over like the unit cardinal directions, X, Y, and Z from negative one to one, you also end up with that same problem, a bias, actually not at the poles, but internal. Got it, got it. I think like the problem with Rlib is like if you can extend that action space in a way that if you call it, is it an instance of a box and it says yes, then it will work out of the box. Okay. But it's not sampling, we don't rely on the sampling logic, it's just like the type of that. Right. Yeah. So I mean, if we can just subclass it, the only method that we really care about currently is just that sample. Yeah. Yeah. I think subclassing would work out of the box. Okay. Cool. It may break it, but I think we can provide fixes. Okay. Yeah. Then we have observation data types, float versus doubles. We haven't really tried this. I think we certainly make assumptions on the torch side for some stuff to be float because like weights by default become doubles and then they will complain at you if you don't have them matched. But I'm curious, why are you interested in doubles? Yeah. So the precision at which we need for some of these orbital updates, right? Float isn't going to give us the precision that we want to do for profit. You got doubles. And that was actually a problem that one of the impetus for why we forked stable base lines is that they made an assumption on comparative floats. Yeah. I think we made that assumption as well, but this is something that we can change and that can actually get reflected on the OSS as well. So if you, for example, dive in and then you have a solution and make contributions back to Rlib, which is going to make both parties happy. So yeah, that's about that question. And then using external hyperparameter tuning packages, definitely yes, Rlib is built on top of Raytune, which, as part of its benefits, supports these hyperparameter tuning packages and algorithms. Obtuna is one of them. There is a link, a great example I'll send over. Pultene tensor board integration, yes, but that's not the only kind of experimental tracking we do. We have weight and biases, which is something people actually use these days. The other questions were, how is it easy to extract policy value functions? Pretty easy. You have access to your models and we provide a callback mechanism where you can tap into every step of your training, whether it's during sampling or after training or in the middle of training, some phases. So we have hooks for them, so you can access your model, your replay buffer, everything that you want to log, and then you pass it up to the original logger, which could be TensorFlow or OneDB in this case. So to touch on that for a second, so I would need to log my replay buffer or log my actor credit model. For this question, one of the things we do is we create visualizations, but we're only operating on the actor number or only on the credit number. And so I'd like to be able to just, in PyTorch, just load that .pt model or something like that. Is that something I'm able to do? Basically, we have these callbacks as like pauses, like breakpoints in your flow, and then you can do whatever you want. If your model is like a torch model, then you can access that particular trial of that model, which would be the actor, and then load up your .pt with that, like your tech point. That's about that question. We can provide more examples if you know more concretely what you want to do, but that can happen down the line. Then we kind of like move those examples, like we usually present those examples in a generic way that can serve others as well, but we can definitely do that down the line if you don't already have it. How are vectorized environments handled under the hood, multiprocessing or something else? This is something that you want to move on to, move to multiprocessing, either multiprocessing or another array process. Right now, the vectorized environments are actually not parallelized. They're like done in a sequence, in a for loop. What is vectorized is once you get your actions back or observation backs, those are batched during inference. So you run one forward pass, you get all of those actions, and then you loop through your environments, apply them in sequence. The multiprocessing thing is something that we're in the design pipeline. There are some complications, for example, one environment finishes early and there's like that aspect that makes it complicated to actually implement. But we're working on it. Can we pass different input arguments to vectorized environments? I think we fully understood this question. Yeah, so one thing that we'd like to be able to do is that if we're training on 20 environments or something like that, pass minute changes to each one to affect them. So we're not just getting very similar, we're trying to span the space addition. I think it's on the vectorized environment side, I don't know, but if you have, that's why we don't like, maybe we don't need vectorized environments because now we're using Ray and every environment can be its own Ray worker. And you can definitely control what each Ray worker kind of environment does. We do that with seeding them. You need them to be on different seeds, for example, to modify their sphechasticity. Some other parameters can be done as well. So based on the worker ID of the worker that is running this environment, you can actually do customization about how you want to pass out. So I understand, when you say worker, is that a single worker within the vectorized environment or is this different? No, it's different. So basically you can think about each worker running your environment on a single CPU node or however resources it needs. Maybe your environment needs four CPU cores on its own, but you have one instance of either a vectorized environment or a normal environment. So you can have many of these, that's what we call workers, and you can scale them horizontally using Ray. That's why Ray is a core part of RLE. And then vectorized environment is just solving a different scenario where spinning up workers becomes your bottleneck and your environment is actually faster. So if that's your use case, then vectorized environment is not actually implemented using multiple processes, it's just using the for loop inside because that's not the problem. The environment is so fast that you can do it in a for loop. Okay. Cool. Thank you. Awesome. But David, where are you at with the RLE stuff? Are you ready to scale it up or are you still kind of in the phase of sorting things out? Yeah, we're still learning. And so the big thing too is we're supporting different types of work, government versus commercial. And we are trying to fit these two kind of heuristic problems that we've defined, but we haven't gotten to the answers on the problems that our customers are actually interested in. So we're kind of solving auxiliary problems that we think will mimic them so that when we can do that transition. So the real important thing to us is the ability for these customized environments. If we can build them up quickly and are able to scale them, that's what I'm interested in. At a high level, basically what you heard from Karosh, do you feel like there's any red flags, yellow flags there? Or do you feel like another conversation needs clarity, where do you currently stand for what you heard today? So at the end of last week, I did start looking into RLE and trying to play around with a few things. But yeah, I mean, I think the thing that I'll have to look at is the sampling because that's what's important to us, but no red flags immediately, but definitely worth looking forward to. Okay. We've got a couple of minutes until the hour. I guess what are your overall thoughts here? We talked about development training, we've talked about tuning, we've talked about serving, we've talked about RL, we've talked about the infrastructure and scaling. Do you guys see this as value? How do you think about Randy's skills so far now that you kind of visualized it today? Oh, I'm sure if you're talking, you're muted by the way. Sorry. So I think it definitely looks beneficial to us. It's definitely something that we could use. I think we had talked last time about what our budget situation was. And it's still unclear what our budget situation is. So one thing that would be useful is getting an idea of what the pricing looks like. And in the event that we really don't have budget, one thing that is appealing is that it's based on this open source. And say, hypothetically, that we did go with Ray as an open source solution, what does it look like if in a year we say that we really want to switch over to a managed solution? Is that an easy thing? Or is it basically like if we chose any other open? You know what I mean? Yeah, it's pretty straightforward. You could take the code and go open source or port it over to AnyScale, so it's bilateral in a way. So that's not a problem. Obviously, AnyScale is just out of the box, which is, you know, do the work for you guys instead of how you maintain it, scale it up, upgrade it, update it, et cetera, et cetera. Also mitigate risks there. Also accelerate adoption. Like, David, I'm not sure if you work at the stable baseline folks, but you would have direct access to Kerhush, the LRL team, to help those cases. So the support along with the infrastructure to allow you guys to scale and develop quickly is where we come into play. But yes, to answer your question, you can do that. We do see folks go that direction early on and switch over. We see folks not go that direction because they want to have the right foundation in place. They don't have to migrate later. Everyone has a little bit of a strategy in how they see the value. And yeah, so a lot of it's going to be dependent on what, if any, budget we're able to acquire. And then I also want to do a little bit more research. Or if you have more information on the integrations that you support and things like experiment tracking model registries, maybe, I know we didn't get a chance to talk about that, but I'm interested in that, maybe in a follow-up email. Yeah. We owe you guys a lot. We owe you answers to David's questions in written form, which I think we have those for some of those little reactor integrations. I think Alex will help you from an operations standpoint to also see the deployment, stuff like how that works, AWS VPC, you guys on the data plan, you know, the control plan would speed up those fault-tolerant performance clusters on your behalf. And then from there, I think maybe Alex, I don't know sure if you control the budget, but just having that conversation at the price of any skill from both the support standpoint, platform standpoint, you know, with different skews available that you can bundle up. So having that, you know, clear to you what that costs and the, or sorry, that costs investments that you'd like to make to solve some of these issues is what we can have in parallel of this. But so that's a follow-up here, but Alex, what are your thoughts on next steps? It seems like there's a lot of directions here. Our perspective is, you know, we have another conversation and clear up any questions you have. I'm not sure who else is involved in this on your guys' side, but that's discussion in parallel of maybe like a budget pricing discussion. So tech plus pricing, two separate conversations is what I'm thinking. But in our world, if everything looks good for you guys, after all those two conversations, we'd love to get in your hands, right? At least test it, compare and contrast what you're doing today, if you're looking at other tools out there. I know you guys are still early, like, you know, we migrate a lot of people from SageMaker, right? So just having a feeling of what it feels like to develop inside any scale for something like that, or the operational side of that, Alex, from your perspective, I think that'd be impactful, but I don't think we're just there yet, but that is, we're open to that if you guys are open to that as well, but I just think there's some clarity on the tech side that we want to seek out on the pricing and budget side. And our director of data science, Dylan, I can't remember if he was on the last call. I think he was invited by, I think he might have. He was invited to this one too, and he had to drop the last minute, but if you could send just like a high level, even if I understand pricing is extremely variable, but just a, like a high level, like scale of what your pricing looks like roughly to, and include Dylan on that email, that would be extremely useful. And then we can go from there. Go from there. Yeah. I'm also, if you guys are okay with that, I'll send this recording. Yeah. Okay. Cool. Anything else? We're right over time. We do have to run another meeting. Any last questions, comments, concerns? Nope. All right. Awesome. Nice to meet you, David. Thanks, Alex. All right. See you guys.