Hey Alex. Hey Anthony, how are you? Good, what about you? Doing all right. Good to hear, good to hear. How's San Francisco, has it been raining the last couple days? Yesterday, I managed to avoid most of it, but it's like just finished raining when I left the office, basically. Okay, I'm gonna be out there next week, next few weeks, and then it's calm here in Sacramento, so like having to commute is the, I'm not, I hope it doesn't, it doesn't rain pretty much in the, during those times. Hey Noah. Hi, how's it going? Hey Noah. Pretty well, how are you? Oh cool, all right. Hey Brent. Hey there. All right, cool. Yeah, we are actually waiting for Amoog, so yeah, there we go. Hey Amoog. Hey, hey guys. Yeah, so yeah, basically just, just like we update from like the engineering side, so I'll be handing off like the DRI for sewer AI to Amoog. Amoog has a lot of experience with inference and batch processing. That being said, I still prepared that demo, which I would like to showcase, so let me share my screen. And just quickly while you set that up, Anthony, Noah, should we expect Jun to join as well, or would just be you? No, he's, he's outside today. Okay, cool. Well, not cool. Okay, understood. Okay, so we took the liberty of setting up a workspace in your account using the support access feature, and that way you can play around with the code, you can run it yourself. So what we wanted to do is just like create like a simple script that would be representative of your workload of what we talked about during our last meeting. How did you remember all the parameters I had? We had a recording. I see. Yeah, but I feel like this is going to be like pretty representative. So I use that example video you have sent, the data. So I have created a bucket with a thousand of those videos for this demo, running 16 of those. So one thing to highlight here is that we are using the new Ray dataset streaming executor, which will be available in Ray 2.3. I am running a nightly version of Ray here, which has access to that. And the streaming executor allows us to very efficiently do this workload without having to load all the data into memory at the same time, which allows us to avoid issues with running out of memory or spilling to disk. Got it. Yeah. So what we do here is we create a dataset pipeline by defining a load clip method. So what's going to happen is that we read the binary files. So those are our videos from S3. We then define the load clip. So this is very similar to what you had. We are using d-card. We are reading the bytes directly into the video reader, and then we just partition it into batches of images and we yield those images in the form of NumPy arrays. And this is what we map the dataset to. So we have the decoded dataset, which is the binary dataset, map batches. We pass the function, and that's it for this. And I use that d-e-t-r model so we can create an actor to do inference. So this is very similar. So before we specify the function here, we create an actor. The difference is that the actor is initialized once, so you do not have to initialize the model over and over again. And this is also very similar to what we have showed. We just pass the tensors to the model and we return the predictions. And we also use map batches here. So I can just run it. And the cluster is composed of eight M5 Forex large nodes and eight G5 GPU nodes. So I run a benchmark, so a thousand images, sorry, a thousand videos, a thousand 50 megabytes videos takes around an hour to fully process. And as we get to the inference part, we'll be able to see the GPU utilization increase. And I was able to get around 90, 95% GPU utilization once the first batch is starting to be processed. That's incredible. Sorry, maybe I missed it. How did you retrieve? Oh, right, you didn't have to. I was going to ask how you retrieved the model weights, but then I realized you loaded it from a pre-trained. Yeah, from Hugging Face. I'm just wondering, instead of having every GPU node have to download the weights independently, would it make sense to put the weights in object memory one time and then have each node read from that? Yeah, so I mean, if they are not too big, then you can of course do that. If the weights are like many gigabytes, I would have advice against that. In that case, better to probably just download it into like a local directory on each node and read from that. Yeah, got it. So yeah, we can see here. So yeah, we are done. So basically, thanks to the streaming execution, it will read the files, then it will immediately pass the read files to be decoded, and then the decoded images are going to be passed to the model in France, and hopefully we'll be able to see. There we go, node GPU. You can see that the GPU utilization is 8698 memory saturated. So if you have like a large amount of videos, this is what you are going to see all the time, like almost maximum GPU saturation. Got it. Well, this is really impressive, and I don't know what to say. I've never had a software vendor do this kind of work. Can you leave this workspace for me? Yeah, of course. That's why we created a new account. If you want to get rid of your temporary bucket, I can use my own videos, but otherwise, yeah. Yeah. So one other thing I need to highlight here. So there is unfortunately a bug slash oversight in data set that's also going to be present in 2.3. So we have fixed that bug. So this is fixed on the nightly version, and this is also present in the cluster environment. The fix is present in the cluster environment I've built. So you can see that we installed the latest nightly wheel here. This will be fixed completely in Ray 2.4, but for now I would recommend using nightly. Okay. That makes sense. Yeah, that's pretty much it. Right. I will start playing around with this. This is very helpful. For sure. Oh, I guess my question is, you know, obviously new videos come in all the time. What's sort of the recommended way to update this data set? I think you just update the S3 bucket. Oh, I see. It's based on that. So one way that I've seen customers do this is using a Lambda function on the AWS side. So when a new video shows up in the S3 bucket, you trigger the Lambda function, the Lambda function. This all gets kicked off by a customer API request through our platform. Okay. We don't actually run inference immediately when the video comes to the system because inference that we charge for inference. Some people just want to store their videos on our system and not have us do work on them. So it's actually a separate kickoff. I can just tie it to the customer endpoint to have a kickoff. And then with any scale SDK, you can trigger any scale job. Or you could have a job, you know, if you know any files in this bucket that showed up over the last 24 hours. If you have it set up on a schedule, you can do it that way as well. Just a simple cron job. That should be in your scale console. You should see a schedules option. So you could also that's another common way for batch inference. It's just every 24 hours, every once a week, however often you want to do it. That's just another way to have that capability. All right. This is, again, very, very neat. I have a few Ray questions that if you guys are okay with. Ask away. All right. This one, I think... Oh, do you mind if I share? Yep. Let me just terminate the workspace. Of course, when you started, like everything is still going to be there, so you can just play around with it. Right. Right. There you go. All right. So if I have a simple task like this, and this guy's job is just to either load a file into object memory from the local cache. If it doesn't exist there, it gets it from S3. And if it's not in the local cache, it gets the object and then calls to save the local cache task to save it to the local cache. It doesn't have to reach from S3 again. Now, I don't wait on save the local cache. I don't right get it, because I want this object to return immediately without waiting on this. The issue here, though, is because I need to send this object both to the color of this function and to save the local cache, I can't... What am I saying? I end up with a reference that points to a reference. And this happens frequently with this sort of thing. And so, maybe this wasn't the right example. No, I don't have one here. Basically, anytime I need to both return an object and pass off. Sorry for being confused. I remember what I wanted to ask. My problem here is that when I read this object, this is now just in memory, and then I pass it here, it is going to build a reference to object memory and send it here. But then when I return it, I feel like it's going to build a second reference to the same thing to return to the color. And so, I wasn't sure how to navigate that situation. I mean, these objects could potentially be 100 megabytes. It's not a crisis if there are two references. Yeah. So, I think here, basically, all you need to do is just in line 640, right before 649, you can do a rate output, which will put the object into the object store once, and then now you can pass that reference wherever you need to. Right. And then the issue with that is now, instead of returning the object, I'm returning the reference to the object. So now, it's a reference to a reference. And so, sometimes, I've just been doing really hacky code. So, when I forget how many layers deep the reference is, I'll just keep looping until I can't get it anymore. So, it'll be cool if there was a resolve reference until you get to an object method or something. I don't know. This may just all because I'm very new to Ray and I'm using it wrong, but I keep running into this sort of issue. So, one thing you can do is you can check if an object is already an instance of Ray object reference. And in that case, you may not just put it into the object store again. That's one suggestion I can give. Yeah. I mean, I do that globally. Just in that one function, it sort of just had that duplicate. Yeah. I don't think it's that big a deal. I was just curious if there's a better way to handle it. Yeah. So, I think Ray should automatically resolve the reference that's being passed into any task. But in this case, I guess it's being returned, right? And you're like, yeah. Okay. Got it. Yeah. I don't think there's any utility to do this nested unwrapping thing. So, I think, yeah, either what you have or what Antoni said, it would just work fine. All right. Cool. So, just to reiterate what Antoni's suggestion was, mostly from my own education, right before you did do the Ray put, I'd see if it's already a reference to that idea. And then if it's not, then put it. And if it is, then skip the put as one option. Does that make sense, Noah? Yeah, it does. Okay. The other question I had is how do I get Ray to stop complaining about relaunching thousands of tasks? Like, it's not a problem. It just gives me error messages. Do you have an example of a error message? Let me see if I can find one. Well, I guess it's a warning, like warning 1,024 Python workers have been started on node, whatever. I mean, it's intended behavior in some cases. I'm just wondering if there's a way to disable it. Yeah. I think just the standard, however you would disable like standard Python warnings, like you could just do that. Yeah. I don't think, yeah, there's nothing really like, yeah, I don't think it was like specific configuration within Ray to disable that, but I guess I just wasn't sure what process was growing the error. It was like the master Ray scheduler or my God, I think this should be coming from like the driver process. So yeah. So using just like the regular, like Python warning filters should work, I think. Got it. All right. Well, I, I again, super appreciate this inference code you created and I'm going to try it out immediately and thanks for answering my questions. Cool. And I'll hand it over to Brent or Anthony, if you have any other questions, but I'm curious from, from my perspective, Noah, just curious what, what other items or what else you have left to kind of test out just or for us to kind of formulate like some milestones on our side as well. I'm going to see if I can get, you know, the, our actual production inference process running and if possible, just let it go for a couple of days and, and monitor it pretty much to production and, and, and make sure everything goes smoothly. Okay, cool. And during those two days, like what, what is it that you're essentially looking to say like, Hey, this one smoothly versus like when it goes smoothly? Well, certainly, you know, jobs completing but also you know, I'll tag all the EC2 machines, get the, the cost you know, divided by the non-videos and compare it to the, our calculations from before. Got it. Cool. Yeah. Just out of curiosity, what were you running the AWS batch? Cause, cause I think you said that it was only running on one instance at a time, correct? Like, like, yeah, one, one, one, one P3, P3 2x large, which is a V100 GPU, which is, which would be overkill for this inference job, except it's a long story, but basically the old, the old version of the Formable Jeter, not the Hugging Face one, but the original one had a custom compiled operator that only worked on certain GPUs. Anyway, now I think we can use a cheaper one, but yeah. Okay. Cool. Cool. And then with that I know we had initially talked about having a sync tomorrow, but would it, it might make more sense if we set up time for Wednesday, would that work for you, Noah? We could have that as a sync and of course we're available on Slack and you know, any ad hoc meetings we need. Sure. That works. Cool. How's 130 Pacific work for you? Yeah, afternoons are great. Fantastic. Okay. So I'll go ahead and send that over and yeah, feel free to reach out to us if there's any other questions, but aside from that, Brent, Anthony, Amog, anything else? I just have one quick question though. Like how frequently would these inference jobs be run and like on what data size? It varies, but typically a few hundred a day, but there, there are, there are times when someone will dump thousands at once. So it's really spiky. Got it. So a few, sorry, a few hundred videos a day, a few hundred jobs a day. A few hundred videos. Okay. Okay. Got it. Got it. Each video has like 50 megabytes, something like that. No, they're usually bigger. The one I sent to you was a fairly low res small one. They're usually considerably bigger. I just wanted to fit into the Slack easy. Got it. Okay. Cool. All right. Anything else we might be able to provide today, Noah? Yeah, that'll do it. Thanks a lot. Awesome. Feel free to reach out with any questions. All right. Great. All right. Thank you. Have a good weekend. Bye. Bye.