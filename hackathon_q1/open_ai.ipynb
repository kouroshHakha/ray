{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai.embeddings_utils import cosine_similarity, get_embedding\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key is used:  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\n"
     ]
    }
   ],
   "source": [
    "\n",
    "openai.api_key = \"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\"\n",
    "print(\"key is used: \", openai.api_key)\n",
    "\n",
    "model = \"text-embedding-ada-002\"\n",
    "account_name = \"sewerai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if this works for out longer text\n",
    "with open(f\"./text_{account_name}.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hey Anthony, how are you? Good, what about you? Doing all right Good to hear, good to hear How's San Francisco, has it been raining the last couple days? Yesterday, I managed to avoid most of it, but it's like just finished raining when I left the office, basically Okay, I'm gonna be out there next week, next few weeks, and then it's calm here in Sacramento, so like having to commute is the, I'm not, I hope it doesn't, it doesn't rain pretty much in the, during those times\",\n",
       " \"Hi, how's it going? Hey Noah Pretty well, how are you? Oh cool, all right Yeah, we are actually waiting for Amoog, so yeah, there we go Yeah, so yeah, basically just, just like we update from like the engineering side, so I'll be handing off like the DRI for sewer AI to Amoog\",\n",
       " \"Amoog has a lot of experience with inference and batch processing That being said, I still prepared that demo, which I would like to showcase, so let me share my screen And just quickly while you set that up, Anthony, Noah, should we expect Jun to join as well, or would just be you? No, he's, he's outside today Okay, so we took the liberty of setting up a workspace in your account using the support access feature, and that way you can play around with the code, you can run it yourself\",\n",
       " 'So what we wanted to do is just like create like a simple script that would be representative of your workload of what we talked about during our last meeting How did you remember all the parameters I had? We had a recording Yeah, but I feel like this is going to be like pretty representative So I use that example video you have sent, the data',\n",
       " 'So I have created a bucket with a thousand of those videos for this demo, running 16 of those So one thing to highlight here is that we are using the new Ray dataset streaming executor, which will be available in Ray 2 I am running a nightly version of Ray here, which has access to that And the streaming executor allows us to very efficiently do this workload without having to load all the data into memory at the same time, which allows us to avoid issues with running out of memory or spilling to disk',\n",
       " \"So what we do here is we create a dataset pipeline by defining a load clip method So what's going to happen is that we read the binary files So those are our videos from S3 We then define the load clip\",\n",
       " 'So this is very similar to what you had We are reading the bytes directly into the video reader, and then we just partition it into batches of images and we yield those images in the form of NumPy arrays And this is what we map the dataset to So we have the decoded dataset, which is the binary dataset, map batches',\n",
       " \"We pass the function, and that's it for this And I use that d-e-t-r model so we can create an actor to do inference So before we specify the function here, we create an actor The difference is that the actor is initialized once, so you do not have to initialize the model over and over again\",\n",
       " 'And this is also very similar to what we have showed We just pass the tensors to the model and we return the predictions And we also use map batches here So I can just run it',\n",
       " \"And the cluster is composed of eight M5 Forex large nodes and eight G5 GPU nodes So I run a benchmark, so a thousand images, sorry, a thousand videos, a thousand 50 megabytes videos takes around an hour to fully process And as we get to the inference part, we'll be able to see the GPU utilization increase And I was able to get around 90, 95% GPU utilization once the first batch is starting to be processed\",\n",
       " \"How did you retrieve? Oh, right, you didn't have to I was going to ask how you retrieved the model weights, but then I realized you loaded it from a pre-trained I'm just wondering, instead of having every GPU node have to download the weights independently, would it make sense to put the weights in object memory one time and then have each node read from that? Yeah, so I mean, if they are not too big, then you can of course do that If the weights are like many gigabytes, I would have advice against that\",\n",
       " \"In that case, better to probably just download it into like a local directory on each node and read from that So yeah, we can see here So basically, thanks to the streaming execution, it will read the files, then it will immediately pass the read files to be decoded, and then the decoded images are going to be passed to the model in France, and hopefully we'll be able to see You can see that the GPU utilization is 8698 memory saturated\",\n",
       " \"So if you have like a large amount of videos, this is what you are going to see all the time, like almost maximum GPU saturation Well, this is really impressive, and I don't know what to say I've never had a software vendor do this kind of work Can you leave this workspace for me? Yeah, of course\",\n",
       " \"That's why we created a new account If you want to get rid of your temporary bucket, I can use my own videos, but otherwise, yeah So one other thing I need to highlight here So there is unfortunately a bug slash oversight in data set that's also going to be present in 2\",\n",
       " \"So we have fixed that bug So this is fixed on the nightly version, and this is also present in the cluster environment The fix is present in the cluster environment I've built So you can see that we installed the latest nightly wheel here\",\n",
       " 'This will be fixed completely in Ray 2 4, but for now I would recommend using nightly I will start playing around with this Oh, I guess my question is, you know, obviously new videos come in all the time',\n",
       " \"What's sort of the recommended way to update this data set? I think you just update the S3 bucket So one way that I've seen customers do this is using a Lambda function on the AWS side So when a new video shows up in the S3 bucket, you trigger the Lambda function, the Lambda function This all gets kicked off by a customer API request through our platform\",\n",
       " \"We don't actually run inference immediately when the video comes to the system because inference that we charge for inference Some people just want to store their videos on our system and not have us do work on them So it's actually a separate kickoff I can just tie it to the customer endpoint to have a kickoff\",\n",
       " 'And then with any scale SDK, you can trigger any scale job Or you could have a job, you know, if you know any files in this bucket that showed up over the last 24 hours If you have it set up on a schedule, you can do it that way as well That should be in your scale console',\n",
       " \"You should see a schedules option So you could also that's another common way for batch inference It's just every 24 hours, every once a week, however often you want to do it That's just another way to have that capability\",\n",
       " 'This is, again, very, very neat I have a few Ray questions that if you guys are okay with Oh, do you mind if I share? Yep Let me just terminate the workspace',\n",
       " \"Of course, when you started, like everything is still going to be there, so you can just play around with it So if I have a simple task like this, and this guy's job is just to either load a file into object memory from the local cache If it doesn't exist there, it gets it from S3 And if it's not in the local cache, it gets the object and then calls to save the local cache task to save it to the local cache\",\n",
       " \"It doesn't have to reach from S3 again Now, I don't wait on save the local cache I don't right get it, because I want this object to return immediately without waiting on this The issue here, though, is because I need to send this object both to the color of this function and to save the local cache, I can't\",\n",
       " \"What am I saying? I end up with a reference that points to a reference And this happens frequently with this sort of thing And so, maybe this wasn't the right example No, I don't have one here\",\n",
       " \"Basically, anytime I need to both return an object and pass off I remember what I wanted to ask My problem here is that when I read this object, this is now just in memory, and then I pass it here, it is going to build a reference to object memory and send it here But then when I return it, I feel like it's going to build a second reference to the same thing to return to the color\",\n",
       " \"And so, I wasn't sure how to navigate that situation I mean, these objects could potentially be 100 megabytes It's not a crisis if there are two references So, I think here, basically, all you need to do is just in line 640, right before 649, you can do a rate output, which will put the object into the object store once, and then now you can pass that reference wherever you need to\",\n",
       " \"And then the issue with that is now, instead of returning the object, I'm returning the reference to the object So now, it's a reference to a reference And so, sometimes, I've just been doing really hacky code So, when I forget how many layers deep the reference is, I'll just keep looping until I can't get it anymore\",\n",
       " \"So, it'll be cool if there was a resolve reference until you get to an object method or something This may just all because I'm very new to Ray and I'm using it wrong, but I keep running into this sort of issue So, one thing you can do is you can check if an object is already an instance of Ray object reference And in that case, you may not just put it into the object store again\",\n",
       " \"That's one suggestion I can give I mean, I do that globally Just in that one function, it sort of just had that duplicate I don't think it's that big a deal\",\n",
       " \"I was just curious if there's a better way to handle it So, I think Ray should automatically resolve the reference that's being passed into any task But in this case, I guess it's being returned, right? And you're like, yeah I don't think there's any utility to do this nested unwrapping thing\",\n",
       " \"So, I think, yeah, either what you have or what Antoni said, it would just work fine So, just to reiterate what Antoni's suggestion was, mostly from my own education, right before you did do the Ray put, I'd see if it's already a reference to that idea And then if it's not, then put it And if it is, then skip the put as one option\",\n",
       " \"Does that make sense, Noah? Yeah, it does The other question I had is how do I get Ray to stop complaining about relaunching thousands of tasks? Like, it's not a problem It just gives me error messages Do you have an example of a error message? Let me see if I can find one\",\n",
       " \"Well, I guess it's a warning, like warning 1,024 Python workers have been started on node, whatever I mean, it's intended behavior in some cases I'm just wondering if there's a way to disable it I think just the standard, however you would disable like standard Python warnings, like you could just do that\",\n",
       " \"I don't think, yeah, there's nothing really like, yeah, I don't think it was like specific configuration within Ray to disable that, but I guess I just wasn't sure what process was growing the error It was like the master Ray scheduler or my God, I think this should be coming from like the driver process So using just like the regular, like Python warning filters should work, I think Well, I, I again, super appreciate this inference code you created and I'm going to try it out immediately and thanks for answering my questions\",\n",
       " \"And I'll hand it over to Brent or Anthony, if you have any other questions, but I'm curious from, from my perspective, Noah, just curious what, what other items or what else you have left to kind of test out just or for us to kind of formulate like some milestones on our side as well I'm going to see if I can get, you know, the, our actual production inference process running and if possible, just let it go for a couple of days and, and monitor it pretty much to production and, and, and make sure everything goes smoothly And during those two days, like what, what is it that you're essentially looking to say like, Hey, this one smoothly versus like when it goes smoothly? Well, certainly, you know, jobs completing but also you know, I'll tag all the EC2 machines, get the, the cost you know, divided by the non-videos and compare it to the, our calculations from before Just out of curiosity, what were you running the AWS batch? Cause, cause I think you said that it was only running on one instance at a time, correct? Like, like, yeah, one, one, one, one P3, P3 2x large, which is a V100 GPU, which is, which would be overkill for this inference job, except it's a long story, but basically the old, the old version of the Formable Jeter, not the Hugging Face one, but the original one had a custom compiled operator that only worked on certain GPUs\",\n",
       " \"Anyway, now I think we can use a cheaper one, but yeah And then with that I know we had initially talked about having a sync tomorrow, but would it, it might make more sense if we set up time for Wednesday, would that work for you, Noah? We could have that as a sync and of course we're available on Slack and you know, any ad hoc meetings we need How's 130 Pacific work for you? Yeah, afternoons are great So I'll go ahead and send that over and yeah, feel free to reach out to us if there's any other questions, but aside from that, Brent, Anthony, Amog, anything else? I just have one quick question though\",\n",
       " 'Like how frequently would these inference jobs be run and like on what data size? It varies, but typically a few hundred a day, but there, there are, there are times when someone will dump thousands at once So a few, sorry, a few hundred videos a day, a few hundred jobs a day Each video has like 50 megabytes, something like that The one I sent to you was a fairly low res small one',\n",
       " \"I just wanted to fit into the Slack easy Anything else we might be able to provide today, Noah? Yeah, that'll do it Feel free to reach out with any questions\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the text by dots and also remove the empty spaces at the beginning and ends\n",
    "sentences = [s.strip() for s in text.split(\".\") if s.strip() != \"\"]\n",
    "# remove sentences that are too short (<= 5 words)\n",
    "sentences = [s for s in sentences if len(s.split(\" \")) > 5]\n",
    "\n",
    "# merge every three sentences together consecutively\n",
    "num_sentences = 4\n",
    "sentences = [\" \".join(sentences[i:i+num_sentences]) for i in range(0, len(sentences), num_sentences)]\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0014076"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "price = num_tokens_from_string(text, \"cl100k_base\") * 0.0004 / 1000\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = openai.Embedding.create(input=sentences, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01003703 -0.01099755  0.01954546 ... -0.03798215 -0.01647698\n",
      "  -0.00492504]\n",
      " [-0.02150167 -0.01630599  0.00062996 ... -0.01733699 -0.00911616\n",
      "   0.00310994]\n",
      " [-0.01362329 -0.01803163  0.00830856 ... -0.02676592 -0.00030878\n",
      "  -0.00072571]\n",
      " ...\n",
      " [-0.00906282  0.00057985  0.00765976 ... -0.0192266  -0.03351229\n",
      "  -0.00779402]\n",
      " [-0.01359735  0.00297195  0.00210726 ... -0.0191683  -0.03099668\n",
      "  -0.00031683]\n",
      " [-0.00746622 -0.00252283  0.0072003  ... -0.01543701 -0.01542337\n",
      "  -0.01161867]]\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "data = resp[\"data\"]\n",
    "embs = np.array([np.array(d[\"embedding\"]) for d in data])\n",
    "print(embs)\n",
    "print(len(embs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [\n",
    "    {\"sentence\": sentence, \"embedding\": embedding} for sentence, embedding in zip(sentences, embs)\n",
    "]\n",
    "pd.DataFrame(data).to_pickle(f\"{account_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str = \"text-embedding-ada-002\") -> np.array:\n",
    "    \"\"\"Returns the embedding of a text.\"\"\"\n",
    "    resp = openai.Embedding.create(input=[text], model=model)\n",
    "    emb = np.array(resp[\"data\"][0][\"embedding\"])\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt_for_gpt3(query: str, sentences_emb: np.ndarray, sentences: list,  top_k: int = 10) -> str:\n",
    "    \"\"\"Constructs a prompt for the OpenAI API.\"\"\"\n",
    "    q_emb = get_embedding(query)\n",
    "    print(\"q_emb, \", q_emb.shape)\n",
    "    print(\"sentences_emb, \", sentences_emb.shape)\n",
    "    sim_scores = cosine_similarity(sentences_emb, q_emb)\n",
    "    inds_sorted = np.argsort(sim_scores)[::-1][:top_k]\n",
    "    print(\"inds_sorted, \", inds_sorted)\n",
    "    sentences_sorted = [sentences[i] for i in inds_sorted]\n",
    "    scores_sorted = [sim_scores[i] for i in inds_sorted]\n",
    "\n",
    "    context = \"\\n* \".join(sentences_sorted)\n",
    "    context = \"\\n* \" + context\n",
    "\n",
    "    # context = \"I've been at Slingshot for about six months now. I'm a machine learning engineer. And I typically focus on my background is actually in more along the lines of software development than is machine learning, though I do have a master's in machine learning or in data science. But so I've been focused mostly on the software side of sort of focusing on the deployment side really of how do we take the take the machine learning models and apply them within our within our applications. So awesome. Yeah. And David, over to you. Yeah. So I'm Dave. Can you hear me all? Yeah. Loud and clear. Yeah. So like Alex, I joined Slingshot about seven months ago or something. And yeah, prior to Slingshot, a lot of my work was in ML, but I was focused on the problems from scientific machine learning to image detection and classification. I did a little bit of reinforcement learning a few years ago, but kind of since I've come to Slingshot, that's been my main area of focus and we've been really trying to stand up some capability to answer some questions, questions for sequential decision making.\"\n",
    "\n",
    "\n",
    "    header = \"\"\"Sahil Lavingia is the founder and CEO of Gumroad, and the author of the book The Minimalist Entrepreneur (also known as TME). These are questions and answers by him. Please keep your answers to three sentences maximum, and speak in complete sentences. Stop speaking once your point is made.\\n\\nContext that may be useful, pulled from The Minimalist Entrepreneur:\\n\"\"\"\n",
    "\n",
    "    qa_examples = [\n",
    "        \"\\n\\n\\nQ: How to choose what business to start?\\n\\nA: First off don't be in a rush. Look around you, see what problems you or other people are facing, and solve one of these problems if you see some overlap with your passions or skills. Or, even if you don't see an overlap, imagine how you would solve that problem anyway. Start super, super small.\",\n",
    "\n",
    "        \"\\n\\n\\nQ: Should we start the business on the side first or should we put full effort right from the start?\\n\\nA:   Always on the side. Things start small and get bigger from there, and I don't know if I would ever “fully” commit to something unless I had some semblance of customer traction. Like with this product I'm working on now!\",\n",
    "\n",
    "        \"\\n\\n\\nQ: Should we sell first than build or the other way around?\\n\\nA: I would recommend building first. Building will teach you a lot, and too many people use “sales” as an excuse to never learn essential skills like building. You can't sell a house you can't build!\",\n",
    "\n",
    "        \"\\n\\n\\nQ: Andrew Chen has a book on this so maybe touché, but how should founders think about the cold start problem? Businesses are hard to start, and even harder to sustain but the latter is somewhat defined and structured, whereas the former is the vast unknown. Not sure if it's worthy, but this is something I have personally struggled with\\n\\nA: Hey, this is about my book, not his! I would solve the problem from a single player perspective first. For example, Gumroad is useful to a creator looking to sell something even if no one is currently using the platform. Usage helps, but it's not necessary.\",\n",
    "\n",
    "        \"\\n\\n\\nQ: What is one business that you think is ripe for a minimalist Entrepreneur innovation that isn't currently being pursued by your community?\\n\\nA: I would move to a place outside of a big city and watch how broken, slow, and non-automated most things are. And of course the big categories like housing, transportation, toys, healthcare, supply chain, food, and more, are constantly being upturned. Go to an industry conference and it's all they talk about! Any industry…\",\n",
    "\n",
    "        \"\\n\\n\\nQ: How can you tell if your pricing is right? If you are leaving money on the table\\n\\nA: I would work backwards from the kind of success you want, how many customers you think you can reasonably get to within a few years, and then reverse engineer how much it should be priced to make that work.\",\n",
    "\n",
    "\n",
    "        \"\\n\\n\\nQ: Why is the name of your book 'the minimalist entrepreneur' \\n\\nA: I think more people should start businesses, and was hoping that making it feel more “minimal” would make it feel more achievable and lead more people to starting-the hardest step.\",\n",
    "\n",
    "        \"\\n\\n\\nQ: How long it takes to write TME\\n\\nA: About 500 hours over the course of a year or two, including book proposal and outline.\",\n",
    "\n",
    "        \"\\n\\n\\nQ: What is the best way to distribute surveys to test my product idea\\n\\nA: I use Google Forms and my email list / Twitter account. Works great and is 100% free.\",\n",
    "\n",
    "        \"\\n\\n\\nQ: How do you know, when to quit\\n\\nA: When I'm bored, no longer learning, not earning enough, getting physically unhealthy, etc… loads of reasons. I think the default should be to “quit” and work on something new. Few things are worth holding your attention for a long period of time.\",\n",
    "    ]\n",
    "\n",
    "    prompt = \"\\n\\n\\nQ: \" + query + \"\\n\\nA: \"\n",
    "\n",
    "    full_prompt = header + context + \"\".join(qa_examples)  + prompt\n",
    "\n",
    "    return full_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb,  (1536,)\n",
      "sentences_emb,  (38, 1536)\n",
      "inds_sorted,  [ 2  3  4 16 17 34 21 18 31  5]\n"
     ]
    }
   ],
   "source": [
    "prompt = construct_prompt_for_gpt3(\n",
    "    query=\"What is the use-case?\",\n",
    "    sentences_emb=embs,\n",
    "    sentences=sentences,\n",
    "    top_k=10,\n",
    ")\n",
    "\n",
    "with open(f\"prompt_{account_name}.txt\", \"w\") as f:\n",
    "    f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_completion(prompt: str, model: str = \"text-davinci-003\", max_tokens: int = 100) -> str:\n",
    "    \"\"\"Runs a completion on a prompt.\"\"\"\n",
    "    resp = openai.Completion.create(\n",
    "        prompt=prompt,\n",
    "        engine=model,\n",
    "        temperature=0.0,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"\\n\\n\"],\n",
    "    )\n",
    "\n",
    "    n_tokens = resp[\"usage\"][\"total_tokens\"]\n",
    "    price = n_tokens * 0.006 / 1000\n",
    "    print(f\"Number of tokens used for completion: {n_tokens} (${price})\")\n",
    "    return resp[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009630000000000001"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-calculate the price of the promopt\n",
    "num_tokens = num_tokens_from_string(prompt, \"gpt2\")\n",
    "num_tokens * 0.006 / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens used for completion: 1657 ($0.009942)\n"
     ]
    }
   ],
   "source": [
    "ans = run_completion(prompt)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt_for_gpt35(query: str, sentences_emb: np.ndarray, sentences: list,  top_k: int = 10) -> str:\n",
    "    \"\"\"Constructs a prompt for the OpenAI API.\"\"\"\n",
    "    q_emb = get_embedding(query)\n",
    "    sim_scores = cosine_similarity(sentences_emb, q_emb)\n",
    "    inds_sorted = np.argsort(sim_scores)[::-1][:top_k]\n",
    "    sentences_sorted = [sentences[i] for i in inds_sorted]\n",
    "    scores_sorted = [sim_scores[i] for i in inds_sorted]\n",
    "    \n",
    "    # number the sentences sorted from 1 ... n\n",
    "    sentences_sorted = [\n",
    "        f\"{i+1}. {sentences_sorted[i]}\" for i in range(len(sentences_sorted))\n",
    "    ]\n",
    "\n",
    "    system_context = f\"\"\"\n",
    "        The user is going to present some questions, and you are a helpful assistant that will answer them based on the context that I am providing here. Here is the context:\n",
    "        {sentences_sorted}\n",
    "        Make sure you don't use any information that is not found in this context.\n",
    "        Please also cite the source of the information that you used to answer their question. For example if the context is \n",
    "        1. The sky is blue.\n",
    "        2. The grass is green.\n",
    "        3. The blue sky is beautiful.\n",
    "        and the user asks \"What is the color of the sky?\" you should answer \"The sky is blue [1][2]\"\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_context},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    return messages, sentences_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def run_completion_with_gpt35(messages, sentences_sorted):\n",
    "\n",
    "    resp = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    ans = resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "    n_tokens = resp[\"usage\"][\"total_tokens\"]\n",
    "    price = n_tokens * 0.002 / 1000\n",
    "    print(f\"Number of tokens used for completion: {n_tokens} (${price})\")\n",
    "\n",
    "    matches = re.findall(r\"\\[(\\d+)\\]\", ans)\n",
    "    \n",
    "    citations = []\n",
    "    if matches:\n",
    "        # minus 1 because the sentences are numbered from 1\n",
    "        citations = [int(match)-1 for match in matches]\n",
    "\n",
    "    citations_text = [f\"{sentences_sorted[i]}\" for i in citations]\n",
    "\n",
    "    if citations_text:\n",
    "        return ans + \"\\ncited sources:\\n\" + \"\\n\".join(citations_text) + \"\\nAll sources:\\n\" + \"\\n\".join(sentences_sorted)\n",
    "    \n",
    "    return ans + \"\\nno citations found\\n\" + \"\\n\".join(sentences_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the use-case?\"\n",
    "top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens used for completion: 678 ($0.001356)\n",
      "The context mentions several different topics, but the main use-case is not explicitly stated. However, it seems to involve creating a script and demo to showcase the use of inference and batch processing, using a dataset of videos and a new Ray dataset streaming executor. The system also has a separate kickoff for charging customers for inference, and the recommended way to update the dataset is through a Lambda function triggered by an S3 bucket.\n",
      "no citations found\n",
      "1. Amoog has a lot of experience with inference and batch processing That being said, I still prepared that demo, which I would like to showcase, so let me share my screen And just quickly while you set that up, Anthony, Noah, should we expect Jun to join as well, or would just be you? No, he's, he's outside today Okay, so we took the liberty of setting up a workspace in your account using the support access feature, and that way you can play around with the code, you can run it yourself\n",
      "2. So what we wanted to do is just like create like a simple script that would be representative of your workload of what we talked about during our last meeting How did you remember all the parameters I had? We had a recording Yeah, but I feel like this is going to be like pretty representative So I use that example video you have sent, the data\n",
      "3. So I have created a bucket with a thousand of those videos for this demo, running 16 of those So one thing to highlight here is that we are using the new Ray dataset streaming executor, which will be available in Ray 2 I am running a nightly version of Ray here, which has access to that And the streaming executor allows us to very efficiently do this workload without having to load all the data into memory at the same time, which allows us to avoid issues with running out of memory or spilling to disk\n",
      "4. We don't actually run inference immediately when the video comes to the system because inference that we charge for inference Some people just want to store their videos on our system and not have us do work on them So it's actually a separate kickoff I can just tie it to the customer endpoint to have a kickoff\n",
      "5. What's sort of the recommended way to update this data set? I think you just update the S3 bucket So one way that I've seen customers do this is using a Lambda function on the AWS side So when a new video shows up in the S3 bucket, you trigger the Lambda function, the Lambda function This all gets kicked off by a customer API request through our platform\n"
     ]
    }
   ],
   "source": [
    "messages, sentences_sorted = construct_prompt_for_gpt35(\n",
    "    query=question,\n",
    "    sentences_emb=embs,\n",
    "    sentences=sentences,\n",
    "    top_k=top_k,\n",
    ")\n",
    "\n",
    "ans = run_completion_with_gpt35(messages, sentences_sorted)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "foo = \"There is no information provided in the given context about what David was doing before joining Slingshot; however, we know that David is the reinforcement learning lead at Slingshot (answer[5]), which indicates that he likely had experience in this field before joining the company.\"\n",
    "\n",
    "import re\n",
    "\n",
    "matches = re.findall(r\"\\(answer\\[(\\d+)\\]\\)\", ans)\n",
    "\n",
    "if matches:\n",
    "    citations = [int(match) for match in matches]\n",
    "print(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [\n",
    "    {\"sentence\": sentence, \"embedding\": embedding} for sentence, embedding in zip(sentences, embs)\n",
    "]\n",
    "pd.DataFrame(data).to_pickle(\"slingshot.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02089262, -0.007267  , -0.00231293, ..., -0.02662907,\n",
       "        -0.01382472, -0.00236737],\n",
       "       [-0.00298221, -0.01602289,  0.01458462, ..., -0.01560655,\n",
       "        -0.00608428, -0.02359276],\n",
       "       [-0.01662703,  0.001294  ,  0.01570604, ..., -0.02228808,\n",
       "        -0.00171152, -0.01424473],\n",
       "       ...,\n",
       "       [ 0.00421428,  0.00804832, -0.00570987, ..., -0.02444914,\n",
       "        -0.02899929, -0.00245569],\n",
       "       [-0.00692392, -0.02459385,  0.02098589, ..., -0.01348444,\n",
       "        -0.00452293,  0.00520429],\n",
       "       [ 0.02155856, -0.00307693,  0.00969457, ..., -0.01539545,\n",
       "         0.01119837,  0.00091907]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"slingshot.pkl\")\n",
    "np.array(df[\"embedding\"].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5abf9a257024fa0ae177d32ddc0977bda32aa95f4f2d5d07f829679a9e9e7642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
