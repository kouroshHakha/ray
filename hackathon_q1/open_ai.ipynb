{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai.embeddings_utils import cosine_similarity, get_embedding\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "find: ‘.git’: No such file or directory\n",
      "2023-03-03 11:26:22,865\tINFO worker.py:1360 -- Connecting to existing Ray cluster at address: 10.0.53.95:6379...\n",
      "2023-03-03 11:26:22,880\tINFO worker.py:1548 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale-staging.com/api/v2/sessions/ses_8y3qzfb6xwlr5ltdjk5vcjh7wm/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
      "2023-03-03 11:26:23,726\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_1f2ed9b8d62287241f84a0a8f37e0cb5.zip' (1.81MiB) to Ray cluster...\n",
      "2023-03-03 11:26:23,761\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_1f2ed9b8d62287241f84a0a8f37e0cb5.zip'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.10.9</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 3.0.0.dev0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://console.anyscale-staging.com/api/v2/sessions/ses_8y3qzfb6xwlr5ltdjk5vcjh7wm/services?redirect_to=dashboard\" target=\"_blank\">http://console.anyscale-staging.com/api/v2/sessions/ses_8y3qzfb6xwlr5ltdjk5vcjh7wm/services?redirect_to=dashboard</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='console.anyscale-staging.com/api/v2/sessions/ses_8y3qzfb6xwlr5ltdjk5vcjh7wm/services?redirect_to=dashboard', python_version='3.10.9', ray_version='3.0.0.dev0', ray_commit='8e4f2cc8b675b82a0da4efca4cbea866757edddc', address_info={'node_ip_address': '10.0.53.95', 'raylet_ip_address': '10.0.53.95', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-03-03_10-16-17_730642_140/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-03-03_10-16-17_730642_140/sockets/raylet', 'webui_url': 'console.anyscale-staging.com/api/v2/sessions/ses_8y3qzfb6xwlr5ltdjk5vcjh7wm/services?redirect_to=dashboard', 'session_dir': '/tmp/ray/session_2023-03-03_10-16-17_730642_140', 'metrics_export_port': 8085, 'gcs_address': '10.0.53.95:6379', 'address': '10.0.53.95:6379', 'dashboard_agent_listen_port': 52365, 'node_id': '779f982e15dabd35fe39d6d716a9fe462d464621cf9e44310b31f646'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray.data\n",
    "import ray\n",
    "import ray.cloudpickle as pickle\n",
    "\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"key is used: \", openai.api_key)\n",
    "\n",
    "EMB_MODEL = \"text-embedding-ada-002\"\n",
    "account_name = \"sewerai\"\n",
    "num_sentence_embs = 3\n",
    "min_num_words_per_sentence = 3\n",
    "# question = \"Why was the user impressed?\"\n",
    "question = \"What is their workload? Categorize in batch inference, RLlib, Tune, or Ray serve.\"\n",
    "\n",
    "top_k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey Alex. Hey Anthony, how are you? Good, what about you? Doing all right. Good to hear, good to hea\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN \n",
    "# let's see if this works for out longer text (This is for when we had the entire text)\n",
    "with open(f\"./text_{account_name}.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(_get_read_tasks pid=587, ip=10.0.15.173) /tmp/ray/session_2023-03-03_10-16-17_730642_140/runtime_resources/py_modules_files/_ray_pkg_faeda1fd19281a14/ray/data/datasource/parquet_datasource.py:233: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead\n",
      "(_get_read_tasks pid=587, ip=10.0.15.173)   pq_ds.pieces, **prefetch_remote_args\n",
      "(_get_read_tasks pid=587, ip=10.0.15.173) /tmp/ray/session_2023-03-03_10-16-17_730642_140/runtime_resources/py_modules_files/_ray_pkg_faeda1fd19281a14/ray/data/datasource/parquet_datasource.py:311: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead\n",
      "(_get_read_tasks pid=587, ip=10.0.15.173)   num_files = len(self._pq_ds.pieces)\n",
      "(_get_read_tasks pid=587, ip=10.0.15.173) /tmp/ray/session_2023-03-03_10-16-17_730642_140/runtime_resources/py_modules_files/_ray_pkg_faeda1fd19281a14/ray/data/datasource/parquet_datasource.py:324: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead\n",
      "(_get_read_tasks pid=587, ip=10.0.15.173)   self._pq_ds.pieces[idx]\n",
      "Parquet Files Sample:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Parquet Files Sample: 100%|██████████| 1/1 [00:05<00:00,  5.96s/it]\n",
      "(_get_read_tasks pid=587, ip=10.0.15.173) /tmp/ray/session_2023-03-03_10-16-17_730642_140/runtime_resources/py_modules_files/_ray_pkg_faeda1fd19281a14/ray/data/datasource/parquet_datasource.py:263: FutureWarning: 'ParquetDataset.pieces' attribute is deprecated as of pyarrow 5.0.0 and will be removed in a future version. Use the '.fragments' attribute instead\n",
      "(_get_read_tasks pid=587, ip=10.0.15.173)   np.array_split(self._pq_ds.pieces, parallelism),\n",
      "2023-03-03 11:26:34,362\tWARNING read_api.py:337 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "data = ray.data.read_parquet(\"s3://antoni-test/gong-calls/8269649578171048284.parquet\")\n",
    "df = data.to_pandas()\n",
    "word_segments = pickle.loads(df[\"word_segments\"].iloc[0])\n",
    "segments = pickle.loads(df[\"segments\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Hey,', 'start': 0.08036529680365298, 'end': 0.1406392694063927},\n",
       " {'text': 'Alex.', 'start': 0.20091324200913246, 'end': 0.4821917808219179},\n",
       " {'text': 'Hey,', 'start': 0.5022831050228311, 'end': 1.8283105022831052},\n",
       " {'text': 'Anthony.', 'start': 1.9287671232876715, 'end': 2.270319634703197},\n",
       " {'text': 'How', 'start': 2.5515981735159823, 'end': 2.6721461187214617},\n",
       " {'text': 'are', 'start': 2.7123287671232883, 'end': 2.792694063926941},\n",
       " {'text': 'you?', 'start': 2.8328767123287677, 'end': 2.9333333333333336},\n",
       " {'text': 'Good.', 'start': 3.5762557077625576, 'end': 3.8173515981735164},\n",
       " {'text': 'What', 'start': 3.83744292237443, 'end': 3.9579908675799094},\n",
       " {'text': 'about', 'start': 3.998173515981736, 'end': 4.199086757990869},\n",
       " {'text': 'you?', 'start': 4.219178082191782, 'end': 4.319634703196348},\n",
       " {'text': 'Doing', 'start': 5.961538461538462, 'end': 6.228205128205128},\n",
       " {'text': 'all', 'start': 6.2897435897435905, 'end': 6.412820512820513},\n",
       " {'text': 'right.', 'start': 6.433333333333334, 'end': 6.638461538461539},\n",
       " {'text': 'Good', 'start': 7.86081081081081, 'end': 8.002702702702702},\n",
       " {'text': 'to', 'start': 8.022972972972973, 'end': 8.124324324324323},\n",
       " {'text': 'hear.', 'start': 8.144594594594595, 'end': 8.428378378378378},\n",
       " {'text': 'Good', 'start': 8.651351351351352, 'end': 8.813513513513513},\n",
       " {'text': 'to', 'start': 8.854054054054055, 'end': 8.975675675675676},\n",
       " {'text': 'hear.', 'start': 8.995945945945946, 'end': 9.17837837837838}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_segments[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey,', 'Alex.', 'Hey,', 'Anthony.', 'How', 'are', 'you?', 'Good.', 'What', 'about']\n"
     ]
    }
   ],
   "source": [
    "words = [w[\"text\"] for w in word_segments]\n",
    "timestamps = [(w[\"start\"], w[\"end\"]) for w in word_segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Word(text='Hey,', start=0.08036529680365298, end=0.1406392694063927), Word(text='Alex.', start=0.20091324200913246, end=0.4821917808219179), Word(text='Hey,', start=0.5022831050228311, end=1.8283105022831052), Word(text='Anthony.', start=1.9287671232876715, end=2.270319634703197), Word(text='How', start=2.5515981735159823, end=2.6721461187214617), Word(text='are', start=2.7123287671232883, end=2.792694063926941), Word(text='you?', start=2.8328767123287677, end=2.9333333333333336), Word(text='Good.', start=3.5762557077625576, end=3.8173515981735164), Word(text='What', start=3.83744292237443, end=3.9579908675799094), Word(text='about', start=3.998173515981736, end=4.199086757990869)]\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Word:\n",
    "    text: str\n",
    "    start: float\n",
    "    end: float\n",
    "\n",
    "words = [Word(**w) for w in word_segments]\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Word(text='Hey,', start=0.08036529680365298, end=0.1406392694063927), Word(text='Alex.', start=0.20091324200913246, end=0.4821917808219179)]\n",
      "[Word(text='Yesterday,', start=17.68019801980198, end=18.4220297029703), Word(text='I', start=18.442079207920795, end=18.462128712871287), Word(text='managed', start=19.38440594059406, end=19.725247524752476), Word(text='to', start=19.765346534653467, end=19.865594059405943), Word(text='avoid', start=19.945792079207923, end=20.246534653465346), Word(text='most', start=20.326732673267326, end=20.56732673267327), Word(text='of', start=20.607425742574257, end=20.66757425742574), Word(text='it,', start=20.747772277227725, end=20.8480198019802), Word(text='but', start=21.128712871287128, end=21.389356435643563), Word(text=\"it's\", start=21.730198019801982, end=22.251485148514853), Word(text='like', start=22.311633663366337, end=22.913118811881187), Word(text='just', start=23.374257425742574, end=23.574752475247525), Word(text='finished', start=23.63490099009901, end=23.91559405940594), Word(text='raining', start=23.99579207920792, end=24.296534653465347), Word(text='when', start=24.336633663366335, end=24.456930693069307), Word(text='I', start=24.497029702970295, end=24.557178217821782), Word(text='left', start=24.59727722772277, end=24.75767326732673), Word(text='the', start=24.797772277227722, end=24.898019801980197), Word(text='office,', start=24.978217821782177, end=25.218811881188117), Word(text='basically.', start=25.25891089108911, end=25.679950495049503)]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "sentence = []\n",
    "\n",
    "for w in words:\n",
    "    if w.text.endswith((\".\", \"?\", \"!\")):\n",
    "        sentence.append(w)\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "    else:\n",
    "        sentence.append(w)\n",
    "print(sentences[0])\n",
    "print(sentences[10])\n",
    "\n",
    "# write the preprocessed sentences into a file to check\n",
    "preprocessed_sentences_text = \"\\n\\n\".join(\n",
    "    [\" \".join([w.text for w in s]) for s in sentences]\n",
    ")\n",
    "\n",
    "with open(f\"./text_{account_name}_preprocessed.txt\", \"w\") as f:\n",
    "    f.write(preprocessed_sentences_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before removing short sentences:  186\n",
      "after removing short sentences:  186\n",
      "after merging sentences:  62\n",
      "example:\n",
      "Has it been raining the last couple of days? Yesterday, I managed to avoid most of it, but it's like just finished raining when I left the office, basically. I'm going to be out there next week, next few weeks.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN \n",
    "# # split the text by dots and also remove the empty spaces at the beginning and ends\n",
    "# sentences = [s.strip() for s in text.split(\".\") if s.strip() != \"\"]\n",
    "# remove sentences that are too short (<= 5 words)\n",
    "# sentences = [s for s in sentences if len(s.split(\" \")) > 5]\n",
    "print(\"before removing short sentences: \", len(sentences))\n",
    "# short_setences = []\n",
    "# for s in sentences:\n",
    "#     if (len(s)) <= min_num_words_per_sentence:\n",
    "#         print(str(len(s)) + \", \" + \" \".join([w.text for w in s]))\n",
    "#         short_setences.append(str(len(s)) + \", \" + \" \".join([w.text for w in s]))\n",
    "#     else:\n",
    "#         long_sentences.append(s)\n",
    "\n",
    "# with open(f\"./text_{account_name}_short_sentences.txt\", \"w\") as f:\n",
    "#     f.write(\"\\n\\n\".join(short_setences))\n",
    "    \n",
    "sentences = [s for s in sentences if len(s) > min_num_words_per_sentence]\n",
    "print(\"after removing short sentences: \", len(sentences))\n",
    "\n",
    "text_after_removing_short_sentences = \"\\n\\n\".join(\n",
    "    [\" \".join([w.text for w in s]) for s in sentences]\n",
    ")\n",
    "with open(f\"./text_{account_name}_after_removing_short_sentences.txt\", \"w\") as f:\n",
    "    f.write(text_after_removing_short_sentences)\n",
    "\n",
    "\n",
    "# sentences = [\" \".join(sentences[i:i+num_sentences]) for i in range(0, len(sentences), num_sentences)]\n",
    "# merge every num_sentences sentences together consecutively into larger list of words\n",
    "merged_sentences = []\n",
    "for i in range(0, len(sentences), num_sentence_embs):\n",
    "    merged_sentences.append([w for s in sentences[i:i+num_sentence_embs] for w in s])\n",
    "\n",
    "print(\"after merging sentences: \", len(merged_sentences))\n",
    "print(\"example:\")\n",
    "print(\" \".join([w.text for w in merged_sentences[0]]))\n",
    "\n",
    "\n",
    "batched_sentences = [\" \".join([w.text for w in sentence]) for sentence in merged_sentences]\n",
    "txt_after_processing = \"\\n\\n\".join(batched_sentences)\n",
    "\n",
    "with open(f\"./text_{account_name}_processed.txt\", \"w\") as f:\n",
    "    f.write(txt_after_processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013332\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "EMBEDDING_PRICE_PER_1K_TOKENS = 0.0004\n",
    "full_text = \" \".join([w.text for sentence in merged_sentences for w in sentence])\n",
    "price = num_tokens_from_string(full_text, \"cl100k_base\") * EMBEDDING_PRICE_PER_1K_TOKENS / 1000\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = openai.Embedding.create(input=batched_sentences, model=EMB_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02249205 -0.00682723  0.01121285 ... -0.02802549 -0.00293425\n",
      "  -0.00396431]\n",
      " [ 0.00769689  0.00162126  0.02084384 ... -0.02787912 -0.01230193\n",
      "  -0.00448712]\n",
      " [-0.03315589 -0.02292612 -0.01956379 ... -0.01399454  0.00924964\n",
      "   0.01191743]\n",
      " ...\n",
      " [-0.02857278 -0.01168145  0.02249667 ... -0.0343476  -0.01708592\n",
      "  -0.02170577]\n",
      " [ 0.00261423 -0.00328275  0.01330881 ... -0.02321178 -0.00982089\n",
      "  -0.03269072]\n",
      " [-0.0121908  -0.00353185  0.00542579 ... -0.00687333 -0.00187425\n",
      "  -0.0138123 ]]\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "data = resp[\"data\"]\n",
    "embs = np.array([np.array(d[\"embedding\"]) for d in data])\n",
    "print(embs)\n",
    "print(len(embs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Word(text='Has', start=14.148863636363638, end=14.610606060606063),\n",
       " Word(text='it', start=14.911742424242426, end=14.971969696969698),\n",
       " Word(text='been', start=14.992045454545456, end=15.152651515151517),\n",
       " Word(text='raining', start=15.413636363636364, end=15.71477272727273),\n",
       " Word(text='the', start=15.734848484848486, end=16.417424242424243),\n",
       " Word(text='last', start=16.45757575757576, end=16.598106060606064),\n",
       " Word(text='couple', start=16.638257575757578, end=16.798863636363638),\n",
       " Word(text='of', start=16.818939393939395, end=16.85909090909091),\n",
       " Word(text='days?', start=16.87916666666667, end=17.03977272727273),\n",
       " Word(text='Yesterday,', start=17.68019801980198, end=18.4220297029703),\n",
       " Word(text='I', start=18.442079207920795, end=18.462128712871287),\n",
       " Word(text='managed', start=19.38440594059406, end=19.725247524752476),\n",
       " Word(text='to', start=19.765346534653467, end=19.865594059405943),\n",
       " Word(text='avoid', start=19.945792079207923, end=20.246534653465346),\n",
       " Word(text='most', start=20.326732673267326, end=20.56732673267327),\n",
       " Word(text='of', start=20.607425742574257, end=20.66757425742574),\n",
       " Word(text='it,', start=20.747772277227725, end=20.8480198019802),\n",
       " Word(text='but', start=21.128712871287128, end=21.389356435643563),\n",
       " Word(text=\"it's\", start=21.730198019801982, end=22.251485148514853),\n",
       " Word(text='like', start=22.311633663366337, end=22.913118811881187),\n",
       " Word(text='just', start=23.374257425742574, end=23.574752475247525),\n",
       " Word(text='finished', start=23.63490099009901, end=23.91559405940594),\n",
       " Word(text='raining', start=23.99579207920792, end=24.296534653465347),\n",
       " Word(text='when', start=24.336633663366335, end=24.456930693069307),\n",
       " Word(text='I', start=24.497029702970295, end=24.557178217821782),\n",
       " Word(text='left', start=24.59727722772277, end=24.75767326732673),\n",
       " Word(text='the', start=24.797772277227722, end=24.898019801980197),\n",
       " Word(text='office,', start=24.978217821782177, end=25.218811881188117),\n",
       " Word(text='basically.', start=25.25891089108911, end=25.679950495049503),\n",
       " Word(text=\"I'm\", start=27.84810606060606, end=27.968560606060606),\n",
       " Word(text='going', start=28.00871212121212, end=28.149242424242424),\n",
       " Word(text='to', start=28.16931818181818, end=28.2094696969697),\n",
       " Word(text='be', start=28.26969696969697, end=28.390151515151516),\n",
       " Word(text='out', start=28.470454545454544, end=28.59090909090909),\n",
       " Word(text='there', start=28.671212121212122, end=28.912121212121214),\n",
       " Word(text='next', start=29.0125, end=29.25340909090909),\n",
       " Word(text='week,', start=29.373863636363637, end=29.654924242424244),\n",
       " Word(text='next', start=30.17689393939394, end=30.377651515151516),\n",
       " Word(text='few', start=30.457954545454545, end=30.618560606060605),\n",
       " Word(text='weeks.', start=30.658712121212123, end=30.91969696969697)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': \"Has it been raining the last couple of days? Yesterday, I managed to avoid most of it, but it's like just finished raining when I left the office, basically. I'm going to be out there next week, next few weeks.\", 'embedding': array([-0.02249205, -0.00682723,  0.01121285, ..., -0.02802549,\n",
      "       -0.00293425, -0.00396431]), 'start': 14.148863636363638, 'end': 30.91969696969697}, {'text': \"And then it's calm here in Sacramento. So like having to commute is the, I'm not, I hope it doesn't, it doesn't rain pretty much in the, during those times. Hi, how's it going?\", 'embedding': array([ 0.00769689,  0.00162126,  0.02084384, ..., -0.02787912,\n",
      "       -0.01230193, -0.00448712]), 'start': 31.281278538812785, 'end': 50.35641025641026}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sentence_data = [\n",
    "    {\n",
    "    \"text\": \" \".join([w.text for w in sentence]), \n",
    "    \"embedding\": embedding,\n",
    "    \"start\": sentence[0].start,\n",
    "    \"end\": sentence[-1].end,\n",
    "    } \n",
    "    for sentence, embedding in zip(merged_sentences, embs)\n",
    "]\n",
    "print(sentence_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sentence_data).to_pickle(f\"{account_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str = \"text-embedding-ada-002\") -> np.array:\n",
    "    \"\"\"Returns the embedding of a text.\"\"\"\n",
    "    resp = openai.Embedding.create(input=[text], model=model)\n",
    "    emb = np.array(resp[\"data\"][0][\"embedding\"])\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN\n",
    "def construct_prompt_for_gpt3(query: str, sentences_emb: np.ndarray, sentences: list,  top_k: int = 10) -> str:\n",
    "    \"\"\"Constructs a prompt for the OpenAI API.\"\"\"\n",
    "    q_emb = get_embedding(query, model=EMB_MODEL)\n",
    "    print(\"q_emb, \", q_emb.shape)\n",
    "    print(\"sentences_emb, \", sentences_emb.shape)\n",
    "    sim_scores = cosine_similarity(sentences_emb, q_emb)\n",
    "    inds_sorted = np.argsort(sim_scores)[::-1][:top_k]\n",
    "    print(\"inds_sorted, \", inds_sorted)\n",
    "    sentences_sorted = [sentences[i] for i in inds_sorted]\n",
    "    scores_sorted = [sim_scores[i] for i in inds_sorted]\n",
    "\n",
    "    context = \"\\n* \".join(sentences_sorted)\n",
    "    context = \"\\n* \" + context\n",
    "\n",
    "    # context = \"I've been at Slingshot for about six months now. I'm a machine learning engineer. And I typically focus on my background is actually in more along the lines of software development than is machine learning, though I do have a master's in machine learning or in data science. But so I've been focused mostly on the software side of sort of focusing on the deployment side really of how do we take the take the machine learning models and apply them within our within our applications. So awesome. Yeah. And David, over to you. Yeah. So I'm Dave. Can you hear me all? Yeah. Loud and clear. Yeah. So like Alex, I joined Slingshot about seven months ago or something. And yeah, prior to Slingshot, a lot of my work was in ML, but I was focused on the problems from scientific machine learning to image detection and classification. I did a little bit of reinforcement learning a few years ago, but kind of since I've come to Slingshot, that's been my main area of focus and we've been really trying to stand up some capability to answer some questions, questions for sequential decision making.\"\n",
    "\n",
    "\n",
    "    header = \"\"\"Sahil Lavingia is the founder and CEO of Gumroad, and the author of the book The Minimalist Entrepreneur (also known as TME). These are questions and answers by him. Please keep your answers to three sentences maximum, and speak in complete sentences. Stop speaking once your point is made.\\n\\nContext that may be useful, pulled from The Minimalist Entrepreneur:\\n\"\"\"\n",
    "\n",
    "    qa_examples = [\n",
    "        \"\\n\\n\\nQ: How to choose what business to start?\\n\\nA: First off don't be in a rush. Look around you, see what problems you or other people are facing, and solve one of these problems if you see some overlap with your passions or skills. Or, even if you don't see an overlap, imagine how you would solve that problem anyway. Start super, super small.\",\n",
    "\n",
    "        \"\\n\\n\\nQ: Should we start the business on the side first or should we put full effort right from the start?\\n\\nA:   Always on the side. Things start small and get bigger from there, and I don't know if I would ever “fully” commit to something unless I had some semblance of customer traction. Like with this product I'm working on now!\",\n",
    "\n",
    "        \"\\n\\n\\nQ: Should we sell first than build or the other way around?\\n\\nA: I would recommend building first. Building will teach you a lot, and too many people use “sales” as an excuse to never learn essential skills like building. You can't sell a house you can't build!\",\n",
    "\n",
    "        \"\\n\\n\\nQ: Andrew Chen has a book on this so maybe touché, but how should founders think about the cold start problem? Businesses are hard to start, and even harder to sustain but the latter is somewhat defined and structured, whereas the former is the vast unknown. Not sure if it's worthy, but this is something I have personally struggled with\\n\\nA: Hey, this is about my book, not his! I would solve the problem from a single player perspective first. For example, Gumroad is useful to a creator looking to sell something even if no one is currently using the platform. Usage helps, but it's not necessary.\",\n",
    "\n",
    "        \"\\n\\n\\nQ: What is one business that you think is ripe for a minimalist Entrepreneur innovation that isn't currently being pursued by your community?\\n\\nA: I would move to a place outside of a big city and watch how broken, slow, and non-automated most things are. And of course the big categories like housing, transportation, toys, healthcare, supply chain, food, and more, are constantly being upturned. Go to an industry conference and it's all they talk about! Any industry…\",\n",
    "\n",
    "        \"\\n\\n\\nQ: How can you tell if your pricing is right? If you are leaving money on the table\\n\\nA: I would work backwards from the kind of success you want, how many customers you think you can reasonably get to within a few years, and then reverse engineer how much it should be priced to make that work.\",\n",
    "\n",
    "\n",
    "        \"\\n\\n\\nQ: Why is the name of your book 'the minimalist entrepreneur' \\n\\nA: I think more people should start businesses, and was hoping that making it feel more “minimal” would make it feel more achievable and lead more people to starting-the hardest step.\",\n",
    "\n",
    "        \"\\n\\n\\nQ: How long it takes to write TME\\n\\nA: About 500 hours over the course of a year or two, including book proposal and outline.\",\n",
    "\n",
    "        \"\\n\\n\\nQ: What is the best way to distribute surveys to test my product idea\\n\\nA: I use Google Forms and my email list / Twitter account. Works great and is 100% free.\",\n",
    "\n",
    "        \"\\n\\n\\nQ: How do you know, when to quit\\n\\nA: When I'm bored, no longer learning, not earning enough, getting physically unhealthy, etc… loads of reasons. I think the default should be to “quit” and work on something new. Few things are worth holding your attention for a long period of time.\",\n",
    "    ]\n",
    "\n",
    "    prompt = \"\\n\\n\\nQ: \" + query + \"\\n\\nA: \"\n",
    "\n",
    "    full_prompt = header + context + \"\".join(qa_examples)  + prompt\n",
    "\n",
    "    return full_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb,  (1536,)\n",
      "sentences_emb,  (38, 1536)\n",
      "inds_sorted,  [ 2  3  4 16 17 34 21 18 31  5]\n"
     ]
    }
   ],
   "source": [
    "prompt = construct_prompt_for_gpt3(\n",
    "    query=\"What is the use-case?\",\n",
    "    sentences_emb=embs,\n",
    "    sentences=sentences,\n",
    "    top_k=10,\n",
    ")\n",
    "\n",
    "with open(f\"prompt_{account_name}.txt\", \"w\") as f:\n",
    "    f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN\n",
    "def run_completion(prompt: str, model: str = \"text-davinci-003\", max_tokens: int = 100) -> str:\n",
    "    \"\"\"Runs a completion on a prompt.\"\"\"\n",
    "    resp = openai.Completion.create(\n",
    "        prompt=prompt,\n",
    "        engine=model,\n",
    "        temperature=0.0,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"\\n\\n\"],\n",
    "    )\n",
    "\n",
    "    n_tokens = resp[\"usage\"][\"total_tokens\"]\n",
    "    price = n_tokens * 0.006 / 1000\n",
    "    print(f\"Number of tokens used for completion: {n_tokens} (${price})\")\n",
    "    return resp[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009630000000000001"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-calculate the price of the promopt\n",
    "num_tokens = num_tokens_from_string(prompt, \"gpt2\")\n",
    "num_tokens * 0.006 / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = run_completion(prompt)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_segments(query: str, sentences_emb: np.ndarray, sentences: list,  top_k: int = 10) -> list:\n",
    "    q_emb = get_embedding(query, model=EMB_MODEL)\n",
    "    sim_scores = cosine_similarity(sentences_emb, q_emb)\n",
    "    inds_sorted = np.argsort(sim_scores)[::-1][:top_k]\n",
    "    sentences_sorted = [sentences[i] for i in inds_sorted]\n",
    "    print(\"scores:\")\n",
    "    print(sim_scores[inds_sorted])\n",
    "\n",
    "    return sentences_sorted\n",
    "\n",
    "def construct_prompt_for_gpt35(query: str, sentences_emb: np.ndarray, sentences: list,  top_k: int = 10, messages=None) -> str:\n",
    "    \"\"\"Constructs a prompt for the OpenAI API.\"\"\"\n",
    "\n",
    "    if messages is not None:\n",
    "        if messages[-1][\"role\"] != \"assistant\":\n",
    "            raise ValueError(\"Last message must be from the assistant\")\n",
    "        \n",
    "    sentences_sorted = get_most_relevant_segments(\n",
    "        query, sentences_emb, sentences, top_k=top_k\n",
    "    )\n",
    "    \n",
    "    # number the sentences sorted from 1 ... n\n",
    "    sentences_sorted_text = [\n",
    "        f\"[{i+1}] ({s['start']:.2f}: {s['end']:.2f}) {s['text']}\" for i, s in enumerate(sentences_sorted)\n",
    "    ]\n",
    "\n",
    "    system_context = f\"\"\"\n",
    "        You are provided with random parts of a transcript, along with the timestamp in format of (start: end). Your task is to answer questions based on the information in the provided context. Make sure not to use any outside information. When answering, cite the source of the information that you used, even if you are infering it based on multiple sources. For example, if the context is:\n",
    "\n",
    "        1. John went outside, but Betty stayed at home.\n",
    "        2. It is raining outside.\n",
    "        3. On a rainy day, you need an umbrella.\n",
    "\n",
    "        And the user asks, \"Who needed an umbrella?\" Your answer should be, \"John. Beacuse, it was raining outside [2], and John went outside [1]. so he probably needed an umbrella [3].\" \n",
    "        The end user will not see the numbered list of sentences, so they won't understand your references. However as the application developer, I will be able to provide them references, if you adhere to the bracket format.\n",
    "\n",
    "        Here is what we found to be the most relevent based on the query:\n",
    "        \n",
    "        {sentences_sorted_text}\n",
    "    \"\"\"\n",
    "\n",
    "    if messages is None:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_context},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    else:\n",
    "        system_context = f\"\"\"\n",
    "            Here is what we found to be the most relevent based on the new query:\n",
    "            {sentences_sorted_text}\n",
    "        \"\"\"\n",
    "        messages.append({\"role\": \"system\", \"content\": system_context})\n",
    "        messages.append({\"role\": \"user\", \"content\": query})\n",
    "    return messages, sentences_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def run_completion_with_gpt35(messages, sentences_sorted):\n",
    "\n",
    "    resp = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    assistance_ans = resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "    n_tokens = resp[\"usage\"][\"total_tokens\"]\n",
    "    price = n_tokens * 0.002 / 1000\n",
    "    print(f\"Number of tokens used for completion: {n_tokens} (${price})\")\n",
    "\n",
    "    matches = re.findall(r\"\\[(\\d+)\\]\", assistance_ans)\n",
    "    \n",
    "    citations = []\n",
    "    if matches:\n",
    "        # minus 1 because the sentences are numbered from 1\n",
    "        citations = [int(match)-1 for match in matches]\n",
    "\n",
    "    citations_sentences = [sentences_sorted[i] for i in citations]\n",
    "    citations_text = [\n",
    "        f\"[{citations[i] + 1}] ({row['start']:.2f}: {row['end']:.2f}) {row['text']}\" \n",
    "        for i, row in enumerate(citations_sentences)\n",
    "    ]\n",
    "    all_sources_text = [f\"[{i+1}] ({row['start']:.2f}: {row['end']:.2f}) {row['text']}\" for i, row in enumerate(sentences_sorted)]\n",
    "\n",
    "    if citations_text:\n",
    "        answer = assistance_ans + \"\\ncited sources:\\n\" + \"\\n\".join(citations_text) + \"\\nAll sources:\\n\" + \"\\n\".join(all_sources_text)\n",
    "    else:\n",
    "        answer = assistance_ans + \"\\nMost Relevant parts\\n\" + \"\\n\".join(all_sources_text)\n",
    "    \n",
    "    return answer, assistance_ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is their workload? Categorize in batch inference, RLlib, Tune, or Ray serve.\n"
     ]
    }
   ],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:\n",
      "[0.10303652 0.1018212  0.10061335 0.09968505 0.09868071 0.0982319\n",
      " 0.09806076 0.09746327 0.09696095 0.0966804 ]\n",
      "[{'role': 'system', 'content': '\\n        You are provided with random parts of a transcript, along with the timestamp in format of (start: end). Your task is to answer questions based on the information in the provided context. Make sure not to use any outside information. When answering, cite the source of the information that you used, even if you are infering it based on multiple sources. For example, if the context is:\\n\\n        1. John went outside, but Betty stayed at home.\\n        2. It is raining outside.\\n        3. On a rainy day, you need an umbrella.\\n\\n        And the user asks, \"Who needed an umbrella?\" Your answer should be, \"John. Beacuse, it was raining outside [2], and John went outside [1]. so he probably needed an umbrella [3].\" \\n        The end user will not see the numbered list of sentences, so they won\\'t understand your references. However as the application developer, I will be able to provide them references, if you adhere to the bracket format.\\n\\n        Here is what we found to be the most relevent based on the query:\\n        \\n        [\"[1] (1325.49: 1345.71) There\\'s any other questions but aside from that, Brent Anthony, a mug, anything else. Just one quick question though like how frequently would these inference jobs be run and like on what data size. But typically, a few hundred a day.\", \"[2] (643.20: 665.25) But that\\'s another common way for batch inference is just every 24 hours every once a week, however often you want to want to do it. That\\'s just another way to have that capability. This is again, very, very neat.\", \\'[3] (159.97: 201.82) So I use that example video you have sent as the data. So I have created a bucket with a thousand of those videos for this demo, running 16 of those. So one thing to highlight here is that we are using the new Ray data set streaming executor, which will be available in Ray 2.3, I am running a nightly version of Ray here which has access to that and the streaming executor allows us to very efficiently do this workload without having to load all the data into memory at the same time, which allows us to avoid issues with running out of memory or spilling to disk.\\', \"[4] (303.44: 341.02) And the cluster is composed of eight M5 forex large nodes and eight G5 GPU nodes. So a thousand, I run a benchmark, so a thousand images, sorry, a thousand videos, a thousand 50 megabytes videos takes around an hour to fully process. And as we get to the inference part, we\\'ll be able to see the GPU utilization increase.\", \"[5] (582.50: 600.87) This all gets kicked off by a customer API requests for a platform. Okay, we don\\'t we don\\'t actually run inference immediately when the video comes to the system, because inference, we that we charge for inference. Some people just want to store their videos on our system and not have us to work on them.\", \\'[6] (277.20: 286.04) We just pass the tensors to the model and we return the predictions. And we also use map batches here. So I can just run it.\\', \"[7] (75.94: 90.00) So, yeah, basically just, just like we update from like the engineering side. So I\\'ll be handing off like the DRI for sewer AI to Amok. Amok has a lot of experience with inference and batch processing.\", \\'[8] (119.12: 145.76) Okay, so we took the liberty of setting up a workspace in your account using the support access feature, and that way you can play around with the code. You can run it yourself. So what we wanted to do is just like create like a simple script that would be representative of your workload of what we talked about during our last meeting.\\', \"[9] (1173.20: 1192.55) So I\\'m going to see if I can get, you know, the, our actual production inference process running. And if possible, just let it go for a couple days. And, and monitor it pretty much to production, and, and make sure everything goes smoothly.\", \\'[10] (1218.20: 1252.32) All the easy to machines, get the, the cost. You know, divide divided on videos and compare it to the calculations from before. Yeah, just out of curiosity, what were you running the database batch, because, because I think you said that it was only running on one instance at a time, correct, like like yeah one, one, one, one p three.\\']\\n    '}, {'role': 'user', 'content': 'What is their workload? Categorize in batch inference, RLlib, Tune, or Ray serve.'}]\n",
      "[{'text': \"There's any other questions but aside from that, Brent Anthony, a mug, anything else. Just one quick question though like how frequently would these inference jobs be run and like on what data size. But typically, a few hundred a day.\", 'embedding': array([-0.01636666,  0.00186682,  0.00688422, ..., -0.01471635,\n",
      "       -0.02986915,  0.01255459]), 'start': 1325.4916145181478, 'end': 1345.7100401606426}, {'text': \"But that's another common way for batch inference is just every 24 hours every once a week, however often you want to want to do it. That's just another way to have that capability. This is again, very, very neat.\", 'embedding': array([-0.01111222,  0.02333026,  0.01401164, ..., -0.00714068,\n",
      "       -0.00995245, -0.02469232]), 'start': 643.2, 'end': 665.245112781955}, {'text': 'So I use that example video you have sent as the data. So I have created a bucket with a thousand of those videos for this demo, running 16 of those. So one thing to highlight here is that we are using the new Ray data set streaming executor, which will be available in Ray 2.3, I am running a nightly version of Ray here which has access to that and the streaming executor allows us to very efficiently do this workload without having to load all the data into memory at the same time, which allows us to avoid issues with running out of memory or spilling to disk.', 'embedding': array([-0.03150314, -0.00815934,  0.01668731, ..., -0.01504268,\n",
      "       -0.02587454, -0.02536414]), 'start': 159.96875, 'end': 201.81973293768547}, {'text': \"And the cluster is composed of eight M5 forex large nodes and eight G5 GPU nodes. So a thousand, I run a benchmark, so a thousand images, sorry, a thousand videos, a thousand 50 megabytes videos takes around an hour to fully process. And as we get to the inference part, we'll be able to see the GPU utilization increase.\", 'embedding': array([-0.0074265 ,  0.01157939, -0.00268686, ..., -0.00640857,\n",
      "       -0.01153204, -0.03757891]), 'start': 303.4404006677796, 'end': 341.0196721311475}, {'text': \"This all gets kicked off by a customer API requests for a platform. Okay, we don't we don't actually run inference immediately when the video comes to the system, because inference, we that we charge for inference. Some people just want to store their videos on our system and not have us to work on them.\", 'embedding': array([ 0.00337242, -0.01572644, -0.00217464, ..., -0.02077989,\n",
      "       -0.02006191, -0.00249738]), 'start': 582.5012048192772, 'end': 600.8733524355301}, {'text': 'We just pass the tensors to the model and we return the predictions. And we also use map batches here. So I can just run it.', 'embedding': array([-0.01843359, -0.01603615,  0.00897705, ..., -0.00665287,\n",
      "       -0.03489595, -0.02489333]), 'start': 277.20039370078734, 'end': 286.0419388830348}, {'text': \"So, yeah, basically just, just like we update from like the engineering side. So I'll be handing off like the DRI for sewer AI to Amok. Amok has a lot of experience with inference and batch processing.\", 'embedding': array([-0.00757112, -0.01336038, -0.01161374, ..., -0.0255939 ,\n",
      "       -0.02177665, -0.00392642]), 'start': 75.94059829059829, 'end': 89.99952153110047}, {'text': 'Okay, so we took the liberty of setting up a workspace in your account using the support access feature, and that way you can play around with the code. You can run it yourself. So what we wanted to do is just like create like a simple script that would be representative of your workload of what we talked about during our last meeting.', 'embedding': array([-0.00339319, -0.00099856,  0.01267118, ..., -0.00574152,\n",
      "        0.00183638,  0.000681  ]), 'start': 119.12032640949556, 'end': 145.75992907801418}, {'text': \"So I'm going to see if I can get, you know, the, our actual production inference process running. And if possible, just let it go for a couple days. And, and monitor it pretty much to production, and, and make sure everything goes smoothly.\", 'embedding': array([-0.03938014, -0.00725424,  0.01316126, ...,  0.00287579,\n",
      "       -0.01782469, -0.00034371]), 'start': 1173.2, 'end': 1192.5541202672607}, {'text': 'All the easy to machines, get the, the cost. You know, divide divided on videos and compare it to the calculations from before. Yeah, just out of curiosity, what were you running the database batch, because, because I think you said that it was only running on one instance at a time, correct, like like yeah one, one, one, one p three.', 'embedding': array([-0.0058723 , -0.01048798,  0.02013362, ..., -0.02234307,\n",
      "       -0.00615539, -0.01207603]), 'start': 1218.2, 'end': 1252.3187410586552}]\n"
     ]
    }
   ],
   "source": [
    "messages = None\n",
    "messages, sentences_sorted = construct_prompt_for_gpt35(\n",
    "    query=question,\n",
    "    sentences_emb=embs,\n",
    "    sentences=sentence_data,\n",
    "    top_k=top_k,\n",
    "    messages=messages\n",
    ")\n",
    "print(messages)\n",
    "print(sentences_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens used for completion: 1066 ($0.002132)\n",
      "Based on the provided context, it seems like their workload primarily involves batch inference. RLlib, Tune, and Ray serve are not mentioned.\n",
      "Most Relevant parts\n",
      "[1] (1325.49: 1345.71) There's any other questions but aside from that, Brent Anthony, a mug, anything else. Just one quick question though like how frequently would these inference jobs be run and like on what data size. But typically, a few hundred a day.\n",
      "[2] (643.20: 665.25) But that's another common way for batch inference is just every 24 hours every once a week, however often you want to want to do it. That's just another way to have that capability. This is again, very, very neat.\n",
      "[3] (159.97: 201.82) So I use that example video you have sent as the data. So I have created a bucket with a thousand of those videos for this demo, running 16 of those. So one thing to highlight here is that we are using the new Ray data set streaming executor, which will be available in Ray 2.3, I am running a nightly version of Ray here which has access to that and the streaming executor allows us to very efficiently do this workload without having to load all the data into memory at the same time, which allows us to avoid issues with running out of memory or spilling to disk.\n",
      "[4] (303.44: 341.02) And the cluster is composed of eight M5 forex large nodes and eight G5 GPU nodes. So a thousand, I run a benchmark, so a thousand images, sorry, a thousand videos, a thousand 50 megabytes videos takes around an hour to fully process. And as we get to the inference part, we'll be able to see the GPU utilization increase.\n",
      "[5] (582.50: 600.87) This all gets kicked off by a customer API requests for a platform. Okay, we don't we don't actually run inference immediately when the video comes to the system, because inference, we that we charge for inference. Some people just want to store their videos on our system and not have us to work on them.\n",
      "[6] (277.20: 286.04) We just pass the tensors to the model and we return the predictions. And we also use map batches here. So I can just run it.\n",
      "[7] (75.94: 90.00) So, yeah, basically just, just like we update from like the engineering side. So I'll be handing off like the DRI for sewer AI to Amok. Amok has a lot of experience with inference and batch processing.\n",
      "[8] (119.12: 145.76) Okay, so we took the liberty of setting up a workspace in your account using the support access feature, and that way you can play around with the code. You can run it yourself. So what we wanted to do is just like create like a simple script that would be representative of your workload of what we talked about during our last meeting.\n",
      "[9] (1173.20: 1192.55) So I'm going to see if I can get, you know, the, our actual production inference process running. And if possible, just let it go for a couple days. And, and monitor it pretty much to production, and, and make sure everything goes smoothly.\n",
      "[10] (1218.20: 1252.32) All the easy to machines, get the, the cost. You know, divide divided on videos and compare it to the calculations from before. Yeah, just out of curiosity, what were you running the database batch, because, because I think you said that it was only running on one instance at a time, correct, like like yeah one, one, one, one p three.\n"
     ]
    }
   ],
   "source": [
    "answer, assist_answer = run_completion_with_gpt35(messages, sentences_sorted)\n",
    "print(answer)\n",
    "messages.append({\"role\": \"assistant\", \"content\": assist_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '\\n        You are provided with random parts of a transcript, along with the timestamp in format of (start: end). Your task is to answer questions based on the information in the provided context. Make sure not to use any outside information. When answering, cite the source of the information that you used, even if you are infering it based on multiple sources. For example, if the context is:\\n\\n        1. John went outside, but Betty stayed at home.\\n        2. It is raining outside.\\n        3. On a rainy day, you need an umbrella.\\n\\n        And the user asks, \"Who needed an umbrella?\" Your answer should be, \"John. Beacuse, it was raining outside [2], and John went outside [1]. so he probably needed an umbrella [3].\" \\n        The end user will not see the numbered list of sentences, so they won\\'t understand your references. However as the application developer, I will be able to provide them references, if you adhere to the bracket format.\\n\\n        Here is what we found to be the most relevent based on the query:\\n        \\n        [\"[1] (1325.49: 1345.71) There\\'s any other questions but aside from that, Brent Anthony, a mug, anything else. Just one quick question though like how frequently would these inference jobs be run and like on what data size. But typically, a few hundred a day.\", \"[2] (643.20: 665.25) But that\\'s another common way for batch inference is just every 24 hours every once a week, however often you want to want to do it. That\\'s just another way to have that capability. This is again, very, very neat.\", \\'[3] (159.97: 201.82) So I use that example video you have sent as the data. So I have created a bucket with a thousand of those videos for this demo, running 16 of those. So one thing to highlight here is that we are using the new Ray data set streaming executor, which will be available in Ray 2.3, I am running a nightly version of Ray here which has access to that and the streaming executor allows us to very efficiently do this workload without having to load all the data into memory at the same time, which allows us to avoid issues with running out of memory or spilling to disk.\\', \"[4] (303.44: 341.02) And the cluster is composed of eight M5 forex large nodes and eight G5 GPU nodes. So a thousand, I run a benchmark, so a thousand images, sorry, a thousand videos, a thousand 50 megabytes videos takes around an hour to fully process. And as we get to the inference part, we\\'ll be able to see the GPU utilization increase.\", \"[5] (582.50: 600.87) This all gets kicked off by a customer API requests for a platform. Okay, we don\\'t we don\\'t actually run inference immediately when the video comes to the system, because inference, we that we charge for inference. Some people just want to store their videos on our system and not have us to work on them.\", \\'[6] (277.20: 286.04) We just pass the tensors to the model and we return the predictions. And we also use map batches here. So I can just run it.\\', \"[7] (75.94: 90.00) So, yeah, basically just, just like we update from like the engineering side. So I\\'ll be handing off like the DRI for sewer AI to Amok. Amok has a lot of experience with inference and batch processing.\", \\'[8] (119.12: 145.76) Okay, so we took the liberty of setting up a workspace in your account using the support access feature, and that way you can play around with the code. You can run it yourself. So what we wanted to do is just like create like a simple script that would be representative of your workload of what we talked about during our last meeting.\\', \"[9] (1173.20: 1192.55) So I\\'m going to see if I can get, you know, the, our actual production inference process running. And if possible, just let it go for a couple days. And, and monitor it pretty much to production, and, and make sure everything goes smoothly.\", \\'[10] (1218.20: 1252.32) All the easy to machines, get the, the cost. You know, divide divided on videos and compare it to the calculations from before. Yeah, just out of curiosity, what were you running the database batch, because, because I think you said that it was only running on one instance at a time, correct, like like yeah one, one, one, one p three.\\']\\n    '},\n",
       " {'role': 'user',\n",
       "  'content': 'What is their workload? Categorize in batch inference, RLlib, Tune, or Ray serve.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Based on the provided context, it seems like their workload primarily involves batch inference. RLlib, Tune, and Ray serve are not mentioned.'}]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '\\n        You are provided with random parts of a transcript, along with the timestamp in format of (start: end). Your task is to answer questions based on the information in the provided context. Make sure not to use any outside information. When answering, cite the source of the information that you used, even if you are infering it based on multiple sources. For example, if the context is:\\n\\n        1. John went outside, but Betty stayed at home.\\n        2. It is raining outside.\\n        3. On a rainy day, you need an umbrella.\\n\\n        And the user asks, \"Who needed an umbrella?\" Your answer should be, \"John. Beacuse, it was raining outside [2], and John went outside [1]. so he probably needed an umbrella [3].\" \\n        The end user will not see the numbered list of sentences, so they won\\'t understand your references. However as the application developer, I will be able to provide them references, if you adhere to the bracket format.\\n\\n        Here is what we found to be the most relevent based on the query:\\n        \\n        [\"[1] (1325.49: 1345.71) There\\'s any other questions but aside from that, Brent Anthony, a mug, anything else. Just one quick question though like how frequently would these inference jobs be run and like on what data size. But typically, a few hundred a day.\", \"[2] (643.20: 665.25) But that\\'s another common way for batch inference is just every 24 hours every once a week, however often you want to want to do it. That\\'s just another way to have that capability. This is again, very, very neat.\", \\'[3] (159.97: 201.82) So I use that example video you have sent as the data. So I have created a bucket with a thousand of those videos for this demo, running 16 of those. So one thing to highlight here is that we are using the new Ray data set streaming executor, which will be available in Ray 2.3, I am running a nightly version of Ray here which has access to that and the streaming executor allows us to very efficiently do this workload without having to load all the data into memory at the same time, which allows us to avoid issues with running out of memory or spilling to disk.\\', \"[4] (303.44: 341.02) And the cluster is composed of eight M5 forex large nodes and eight G5 GPU nodes. So a thousand, I run a benchmark, so a thousand images, sorry, a thousand videos, a thousand 50 megabytes videos takes around an hour to fully process. And as we get to the inference part, we\\'ll be able to see the GPU utilization increase.\", \"[5] (582.50: 600.87) This all gets kicked off by a customer API requests for a platform. Okay, we don\\'t we don\\'t actually run inference immediately when the video comes to the system, because inference, we that we charge for inference. Some people just want to store their videos on our system and not have us to work on them.\", \\'[6] (277.20: 286.04) We just pass the tensors to the model and we return the predictions. And we also use map batches here. So I can just run it.\\', \"[7] (75.94: 90.00) So, yeah, basically just, just like we update from like the engineering side. So I\\'ll be handing off like the DRI for sewer AI to Amok. Amok has a lot of experience with inference and batch processing.\", \\'[8] (119.12: 145.76) Okay, so we took the liberty of setting up a workspace in your account using the support access feature, and that way you can play around with the code. You can run it yourself. So what we wanted to do is just like create like a simple script that would be representative of your workload of what we talked about during our last meeting.\\', \"[9] (1173.20: 1192.55) So I\\'m going to see if I can get, you know, the, our actual production inference process running. And if possible, just let it go for a couple days. And, and monitor it pretty much to production, and, and make sure everything goes smoothly.\", \\'[10] (1218.20: 1252.32) All the easy to machines, get the, the cost. You know, divide divided on videos and compare it to the calculations from before. Yeah, just out of curiosity, what were you running the database batch, because, because I think you said that it was only running on one instance at a time, correct, like like yeah one, one, one, one p three.\\']\\n    '}]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:\n",
      "[0.09523204 0.09482079 0.09444534 0.09425361 0.09396167 0.09392101\n",
      " 0.09389108 0.09341508 0.09331822 0.09324886]\n",
      "[{'role': 'system', 'content': '\\n        You are provided with random parts of a transcript, along with the timestamp in format of (start: end). Your task is to answer questions based on the information in the provided context. Make sure not to use any outside information. When answering, cite the source of the information that you used, even if you are infering it based on multiple sources. For example, if the context is:\\n\\n        1. John went outside, but Betty stayed at home.\\n        2. It is raining outside.\\n        3. On a rainy day, you need an umbrella.\\n\\n        And the user asks, \"Who needed an umbrella?\" Your answer should be, \"John. Beacuse, it was raining outside [2], and John went outside [1]. so he probably needed an umbrella [3].\" \\n        The end user will not see the numbered list of sentences, so they won\\'t understand your references. However as the application developer, I will be able to provide them references, if you adhere to the bracket format.\\n\\n        Here is what we found to be the most relevent based on the query:\\n        \\n        [\"[1] (1325.49: 1345.71) There\\'s any other questions but aside from that, Brent Anthony, a mug, anything else. Just one quick question though like how frequently would these inference jobs be run and like on what data size. But typically, a few hundred a day.\", \"[2] (643.20: 665.25) But that\\'s another common way for batch inference is just every 24 hours every once a week, however often you want to want to do it. That\\'s just another way to have that capability. This is again, very, very neat.\", \\'[3] (159.97: 201.82) So I use that example video you have sent as the data. So I have created a bucket with a thousand of those videos for this demo, running 16 of those. So one thing to highlight here is that we are using the new Ray data set streaming executor, which will be available in Ray 2.3, I am running a nightly version of Ray here which has access to that and the streaming executor allows us to very efficiently do this workload without having to load all the data into memory at the same time, which allows us to avoid issues with running out of memory or spilling to disk.\\', \"[4] (303.44: 341.02) And the cluster is composed of eight M5 forex large nodes and eight G5 GPU nodes. So a thousand, I run a benchmark, so a thousand images, sorry, a thousand videos, a thousand 50 megabytes videos takes around an hour to fully process. And as we get to the inference part, we\\'ll be able to see the GPU utilization increase.\", \"[5] (582.50: 600.87) This all gets kicked off by a customer API requests for a platform. Okay, we don\\'t we don\\'t actually run inference immediately when the video comes to the system, because inference, we that we charge for inference. Some people just want to store their videos on our system and not have us to work on them.\", \\'[6] (277.20: 286.04) We just pass the tensors to the model and we return the predictions. And we also use map batches here. So I can just run it.\\', \"[7] (75.94: 90.00) So, yeah, basically just, just like we update from like the engineering side. So I\\'ll be handing off like the DRI for sewer AI to Amok. Amok has a lot of experience with inference and batch processing.\", \\'[8] (119.12: 145.76) Okay, so we took the liberty of setting up a workspace in your account using the support access feature, and that way you can play around with the code. You can run it yourself. So what we wanted to do is just like create like a simple script that would be representative of your workload of what we talked about during our last meeting.\\', \"[9] (1173.20: 1192.55) So I\\'m going to see if I can get, you know, the, our actual production inference process running. And if possible, just let it go for a couple days. And, and monitor it pretty much to production, and, and make sure everything goes smoothly.\", \\'[10] (1218.20: 1252.32) All the easy to machines, get the, the cost. You know, divide divided on videos and compare it to the calculations from before. Yeah, just out of curiosity, what were you running the database batch, because, because I think you said that it was only running on one instance at a time, correct, like like yeah one, one, one, one p three.\\']\\n    '}, {'role': 'user', 'content': 'What is their workload? Categorize in batch inference, RLlib, Tune, or Ray serve.'}, {'role': 'assistant', 'content': 'Based on the provided context, it seems like their workload primarily involves batch inference. RLlib, Tune, and Ray serve are not mentioned.'}, {'role': 'system', 'content': '\\n            Here is what we found to be the most relevent based on the new query:\\n            [\"[1] (845.20: 855.57) And so I wasn\\'t sure how to navigate that situation. I mean, is it. I mean these objects could potentially be you know, 100 megabytes, it\\'s not a crisis if there are two references.\", \"[2] (963.24: 980.83) During, I don\\'t think it\\'s that big a deal I was just curious in terms of better way to handle it. Yeah, so I think like ratio automatically resolve the reference that\\'s being passed into any task. In this case, I guess it\\'s being returned right and you\\'re like yeah.\", \"[3] (773.22: 803.79) What am I saying is I end up with a reference that points to a reference, and this happens frequently with this sort of thing. And so, maybe this wasn\\'t the right example. No, I don\\'t have one here.\", \\'[4] (119.12: 145.76) Okay, so we took the liberty of setting up a workspace in your account using the support access feature, and that way you can play around with the code. You can run it yourself. So what we wanted to do is just like create like a simple script that would be representative of your workload of what we talked about during our last meeting.\\', \"[5] (858.62: 876.06) Yeah, so I think like here. I\\'m basically all you need to do is just like in line 640, like right before 649. You can do like, like, you can do like the rate output, which will like put the object into the object store once and then now you can pass that reference, wherever you need to.\", \"[6] (361.20: 367.80) You didn\\'t have to. I was going to ask how you retrieved the model weights, but then I realized you loaded it from a pre-trained. Yeah, from Hugging Face.\", \"[7] (1047.22: 1080.34) Can you give an example of their message. See if I can find one. I guess it\\'s a warning by warning 1024 Python workers haven\\'t started on node, whatever.\", \"[8] (459.24: 472.97) And I don\\'t know to say I\\'ve never had a software vendor. It\\'s kind of work. Can you can you leave this workspace for me.\", \"[9] (671.36: 721.92) I have some, a few great questions that if you guys are okay with. So you can just play around with it. So, if I, I have a simple task like this, and this, this guy\\'s job is just to either load a file into object memory from the local cache.\", \\'[10] (511.70: 524.61) This will be fixed completely in Ray 2.4. But for now I would recommend using nightly. Okay, that makes sense.\\']\\n        '}, {'role': 'user', 'content': 'You mentioned RLlib? Where do they mention RLlib?'}]\n",
      "[{'text': \"And so I wasn't sure how to navigate that situation. I mean, is it. I mean these objects could potentially be you know, 100 megabytes, it's not a crisis if there are two references.\", 'embedding': array([-0.00034037, -0.00272465,  0.00733092, ..., -0.02485902,\n",
      "       -0.03256087, -0.03822597]), 'start': 845.2, 'end': 855.57343358396}, {'text': \"During, I don't think it's that big a deal I was just curious in terms of better way to handle it. Yeah, so I think like ratio automatically resolve the reference that's being passed into any task. In this case, I guess it's being returned right and you're like yeah.\", 'embedding': array([ 0.00609889,  0.00145274,  0.01317388, ..., -0.02515397,\n",
      "       -0.01297726, -0.04502716]), 'start': 963.2401146131806, 'end': 980.8331658291457}, {'text': \"What am I saying is I end up with a reference that points to a reference, and this happens frequently with this sort of thing. And so, maybe this wasn't the right example. No, I don't have one here.\", 'embedding': array([-0.02379305, -0.01022142,  0.01113451, ..., -0.02586886,\n",
      "       -0.02066618, -0.03318676]), 'start': 773.2200803212852, 'end': 803.7879396984926}, {'text': 'Okay, so we took the liberty of setting up a workspace in your account using the support access feature, and that way you can play around with the code. You can run it yourself. So what we wanted to do is just like create like a simple script that would be representative of your workload of what we talked about during our last meeting.', 'embedding': array([-0.00339319, -0.00099856,  0.01267118, ..., -0.00574152,\n",
      "        0.00183638,  0.000681  ]), 'start': 119.12032640949556, 'end': 145.75992907801418}, {'text': \"Yeah, so I think like here. I'm basically all you need to do is just like in line 640, like right before 649. You can do like, like, you can do like the rate output, which will like put the object into the object store once and then now you can pass that reference, wherever you need to.\", 'embedding': array([-0.02811528,  0.00535463, -0.01081411, ...,  0.0129951 ,\n",
      "       -0.01865032, -0.03215572]), 'start': 858.621052631579, 'end': 876.0564273789649}, {'text': \"You didn't have to. I was going to ask how you retrieved the model weights, but then I realized you loaded it from a pre-trained. Yeah, from Hugging Face.\", 'embedding': array([-0.00281117, -0.00434751,  0.01757311, ..., -0.01315368,\n",
      "       -0.01714162, -0.02934081]), 'start': 361.2, 'end': 367.7959595959596}, {'text': \"Can you give an example of their message. See if I can find one. I guess it's a warning by warning 1024 Python workers haven't started on node, whatever.\", 'embedding': array([-0.01979472, -0.02733623, -0.01142379, ..., -0.01083831,\n",
      "       -0.00926309, -0.04430115]), 'start': 1047.2200573065902, 'end': 1080.3362725450902}, {'text': \"And I don't know to say I've never had a software vendor. It's kind of work. Can you can you leave this workspace for me.\", 'embedding': array([ 0.01873918, -0.02187383,  0.00654084, ..., -0.01966032,\n",
      "        0.01620946, -0.01156247]), 'start': 459.2434056761269, 'end': 472.96537911301857}, {'text': \"I have some, a few great questions that if you guys are okay with. So you can just play around with it. So, if I, I have a simple task like this, and this, this guy's job is just to either load a file into object memory from the local cache.\", 'embedding': array([-0.01054069,  0.01179586,  0.0153994 , ..., -0.0114652 ,\n",
      "       -0.01323998, -0.01210628]), 'start': 671.3603563474388, 'end': 721.9152391546163}, {'text': 'This will be fixed completely in Ray 2.4. But for now I would recommend using nightly. Okay, that makes sense.', 'embedding': array([-0.02003212,  0.00293376,  0.01138312, ...,  0.02459349,\n",
      "       -0.01143049, -0.0203705 ]), 'start': 511.7025125628141, 'end': 524.6093959731544}]\n"
     ]
    }
   ],
   "source": [
    "question2 = \"You mentioned RLlib? Where do they mention RLlib?\"\n",
    "messages, sentences_sorted = construct_prompt_for_gpt35(\n",
    "    query=question2,\n",
    "    sentences_emb=embs,\n",
    "    sentences=sentence_data,\n",
    "    top_k=top_k,\n",
    "    messages=messages\n",
    ")\n",
    "print(messages)\n",
    "print(sentences_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens used for completion: 1753 ($0.0035060000000000004)\n",
      "I apologize for the mistake. After reviewing the context again, I did not find any mention of RLlib.\n",
      "Most Relevant parts\n",
      "[1] (845.20: 855.57) And so I wasn't sure how to navigate that situation. I mean, is it. I mean these objects could potentially be you know, 100 megabytes, it's not a crisis if there are two references.\n",
      "[2] (963.24: 980.83) During, I don't think it's that big a deal I was just curious in terms of better way to handle it. Yeah, so I think like ratio automatically resolve the reference that's being passed into any task. In this case, I guess it's being returned right and you're like yeah.\n",
      "[3] (773.22: 803.79) What am I saying is I end up with a reference that points to a reference, and this happens frequently with this sort of thing. And so, maybe this wasn't the right example. No, I don't have one here.\n",
      "[4] (119.12: 145.76) Okay, so we took the liberty of setting up a workspace in your account using the support access feature, and that way you can play around with the code. You can run it yourself. So what we wanted to do is just like create like a simple script that would be representative of your workload of what we talked about during our last meeting.\n",
      "[5] (858.62: 876.06) Yeah, so I think like here. I'm basically all you need to do is just like in line 640, like right before 649. You can do like, like, you can do like the rate output, which will like put the object into the object store once and then now you can pass that reference, wherever you need to.\n",
      "[6] (361.20: 367.80) You didn't have to. I was going to ask how you retrieved the model weights, but then I realized you loaded it from a pre-trained. Yeah, from Hugging Face.\n",
      "[7] (1047.22: 1080.34) Can you give an example of their message. See if I can find one. I guess it's a warning by warning 1024 Python workers haven't started on node, whatever.\n",
      "[8] (459.24: 472.97) And I don't know to say I've never had a software vendor. It's kind of work. Can you can you leave this workspace for me.\n",
      "[9] (671.36: 721.92) I have some, a few great questions that if you guys are okay with. So you can just play around with it. So, if I, I have a simple task like this, and this, this guy's job is just to either load a file into object memory from the local cache.\n",
      "[10] (511.70: 524.61) This will be fixed completely in Ray 2.4. But for now I would recommend using nightly. Okay, that makes sense.\n"
     ]
    }
   ],
   "source": [
    "answer, assist_answer = run_completion_with_gpt35(messages, sentences_sorted)\n",
    "print(answer)\n",
    "messages.append({\"role\": \"assistant\", \"content\": assist_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "5abf9a257024fa0ae177d32ddc0977bda32aa95f4f2d5d07f829679a9e9e7642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
