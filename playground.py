from ray.rllib.algorithms.crr import CRR, CRR_DEFAULT_CONFIG

data_file = 'rllib/tests/data/pendulum/small.json'
# data_file = 'rllib/tests/data/pendulum/large.json'
config = {
    "env": "Pendulum-v1",
    "input": [data_file],
    # In the files, we use here for testing, actions have already
    # been normalized.
    # This is usually the case when the file was generated by another
    # RLlib algorithm (e.g. PPO or SAC).
    "actions_in_input_normalized": False,
    "clip_actions": True,
    "train_batch_size": 2000,
    "twin_q": True,
    "replay_buffer_config": {"learning_starts": 0},
    "bc_iters": 2,  # 2 BC iters, 2 CQL iters.
    "rollout_fragment_length": 1,
    # Switch on off-policy evaluation.
    "input_evaluation": ["is"],
    "always_attach_evaluation_results": True,
    "evaluation_interval": 2,
    "evaluation_duration": 10,
    "evaluation_config": {
        "input": "sampler",
    },
    "evaluation_parallel_to_training": False,
    "evaluation_num_workers": 2,
    'framework': 'torch',
}

algo = CRR(config=config)
results = algo.train()
