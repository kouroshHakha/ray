deployment_config: {}
model_loading_config:
  model_id: meta-llama/Meta-Llama-3.1-8B-Instruct
engine_kwargs:
  # Llama 3.1's context length is 128k, but we set a lower one to avoid GPU OOM.
  max_model_len: 8192
accelerator_type: A10G
lora_config: null
