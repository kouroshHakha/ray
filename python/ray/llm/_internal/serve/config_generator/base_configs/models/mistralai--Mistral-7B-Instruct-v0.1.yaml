deployment_config:
  autoscaling_config:
    target_ongoing_requests: 16
  max_ongoing_requests: 32

model_loading_config:
  model_id: mistralai/Mistral-7B-Instruct-v0.1

generation_config:
  prompt_format:
    use_hugging_face_chat_template: true
  stopping_sequences: []
  stopping_tokens: []

llm_engine: VLLMEngine

engine_kwargs:
  trust_remote_code: true
  tokenizer_pool_size: 2
  tokenizer_pool_extra_config:
    runtime_env:
      pip: null
  tensor_parallel_size: 1

accelerator_type: A10G


lora_config: null

# Note: mistral 7B config is used as the default config for models outside the default
# model list. Some model may require shorter max model length. This
# VLLM_ALLOW_LONG_MAX_MODEL_LEN will ensure that the model can be loaded even if the
# max model length is longer than the default max model length.
runtime_env:
  env_vars:
    VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
