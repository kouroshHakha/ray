deployment_config:
  autoscaling_config:
    target_ongoing_requests: 16
  max_ongoing_requests: 32

model_loading_config:
  model_id: meta-llama/Meta-Llama-3.1-405B-Instruct-FP8


engine_kwargs:
  # Llama 3.1's context length is 128k, but we set a lower one to avoid GPU OOM.
  max_model_len: 8192
  tensor_parallel_size: 8

accelerator_type: H100

lora_config: null
