deployment_config: {}

model_loading_config:
  model_id: meta-llama/Meta-Llama-3.1-70B-Instruct


engine_kwargs:
  # Llama 3.1's context length is 128k, but we set a lower one to avoid GPU OOM.
  max_model_len: 8192
  tensor_parallel_size: 8

accelerator_type: A100_40G

lora_config: null
