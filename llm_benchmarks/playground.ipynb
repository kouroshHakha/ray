{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from pprint import pprint\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = \"gpt2-xl\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "class DummyContext:\n",
    "    def __enter__(self):\n",
    "        pass\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        pass\n",
    "\n",
    "def get_memory_usage(device):\n",
    "    mem_alloc = torch.cuda.memory_allocated(device)\n",
    "    mem_reserved = torch.cuda.memory_reserved(device)\n",
    "    return mem_alloc, mem_reserved\n",
    "\n",
    "def memory_footprint_gpu(model, batch_size, data_type=torch.float32, device=torch.device('cuda'), mode=\"train\"):\n",
    "    assert mode in (\"train\", \"inference\")\n",
    "    model.to(device)\n",
    "    # Calculate the size of the model parameters\n",
    "    num_parameters = sum(p.numel() for p in model.parameters())\n",
    "    dtype_size = torch.tensor(1, dtype=data_type).element_size()\n",
    "    parameter_memory = num_parameters * dtype_size\n",
    "\n",
    "    # Perform a forward pass to estimate activations size\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    input_text = \"This is a sample text.\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    print(f\"number of tokens: {input_ids.shape[1]}\")\n",
    "\n",
    "    context = torch.no_grad() if mode == \"inference\" else DummyContext()\n",
    "    with torch.no_grad():\n",
    "        # Repeat the input to match the batch size of 1\n",
    "        input_ids_batch = input_ids\n",
    "\n",
    "        # Clear any existing CUDA cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # Monitor memory usage before and after running the model\n",
    "        torch.cuda.synchronize()\n",
    "        mem_before, _ = get_memory_usage(device)\n",
    "        model(input_ids_batch)\n",
    "        torch.cuda.synchronize()\n",
    "        mem_after, _ = get_memory_usage(device)\n",
    "\n",
    "        print(\"mem_before\")\n",
    "        print(mem_before)\n",
    "        print(\"mem_Afeter\")\n",
    "        print(mem_after)\n",
    "\n",
    "        # Calculate the size of activations\n",
    "        activations_memory = (mem_after - mem_before) * batch_size\n",
    "\n",
    "    # Calculate the total memory footprint\n",
    "    total_memory = parameter_memory + activations_memory\n",
    "    infos = {\"parameter_memory\": parameter_memory, \"activation_memory\": activations_memory}\n",
    "    return num_parameters, total_memory, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dtype = torch.float32\n",
    "print(\"[Mode: Train]\")\n",
    "nparams, mem_ftp, infos = memory_footprint_gpu(model, batch_size, data_type=dtype, mode=\"train\")\n",
    "print(f\"Num parameters: {nparams / 1e9:.2f}B, Mem footprint for inference with bsize = {batch_size}: { mem_ftp / 2**30 :.2f} GB\")\n",
    "\n",
    "print(\"[Mode: Inference]\")\n",
    "nparams, mem_ftp, infos = memory_footprint_gpu(model, batch_size, data_type=dtype, mode=\"inference\")\n",
    "print(f\"Num parameters: {nparams / 1e9:.2f}B, Mem footprint for inference with bsize = {batch_size}: { mem_ftp / 2**30 :.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "tensor = torch.randn((1024, 1024, 1024)).to(torch.float16).to(device)\n",
    "# print(torch.cuda.memory_allocated(device))\n",
    "print(f\"cache: {torch.cuda.memory_reserved(device)}\")\n",
    "del tensor\n",
    "gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_reserved(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "layer = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1024, 1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1024, 1)\n",
    ").to(torch.float16).to(device)\n",
    "tensor = torch.randn((1024, 1024, 1024)).to(torch.float16).to(device)\n",
    "torch.cuda.synchronize()  # Force synchronization\n",
    "_, mem_before = get_memory_usage(device)\n",
    "loss = layer(tensor)\n",
    "torch.cuda.synchronize()  # Force synchronization\n",
    "_, mem_after = get_memory_usage(device)\n",
    "print(mem_before)\n",
    "print(mem_after)\n",
    "\n",
    "print(f\"mem_usage: {mem_after - mem_before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tensor\n",
    "del layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def estimate_memory_training(model, sample_input, optimizer_type=torch.optim.Adam, batch_size=1, use_amp=False, device=0):\n",
    "    \"\"\"Predict the maximum memory usage of the model. \n",
    "    Args:\n",
    "        optimizer_type (Type): the class name of the optimizer to instantiate\n",
    "        model (nn.Module): the neural network model\n",
    "        sample_input (torch.Tensor): A sample input to the network. It should be \n",
    "            a single item, not a batch, and it will be replicated batch_size times.\n",
    "        batch_size (int): the batch size\n",
    "        use_amp (bool): whether to estimate based on using mixed precision\n",
    "        device (torch.device): the device to use\n",
    "    \"\"\"\n",
    "    # Reset model and optimizer\n",
    "    model.cpu()\n",
    "    optimizer = optimizer_type(model.parameters(), lr=.001)\n",
    "    a = torch.cuda.memory_allocated(device)\n",
    "    model.to(device)\n",
    "    b = torch.cuda.memory_allocated(device)\n",
    "    model_memory = b - a\n",
    "    model_input = sample_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "    output = model(model_input.to(device)).sum()\n",
    "    c = torch.cuda.memory_allocated(device)\n",
    "    if use_amp:\n",
    "        amp_multiplier = .5\n",
    "    else:\n",
    "        amp_multiplier = 1\n",
    "    forward_pass_memory = (c - b)*amp_multiplier\n",
    "    gradient_memory = model_memory\n",
    "    if isinstance(optimizer, torch.optim.Adam):\n",
    "        o = 2\n",
    "    elif isinstance(optimizer, torch.optim.RMSprop):\n",
    "        o = 1\n",
    "    elif isinstance(optimizer, torch.optim.SGD):\n",
    "        o = 0\n",
    "    elif isinstance(optimizer, torch.optim.Adagrad):\n",
    "        o = 1\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer. Look up how many moments are\" +\n",
    "            \"stored by your optimizer and add a case to the optimizer checker.\")\n",
    "    gradient_moment_memory = o*gradient_memory\n",
    "    total_memory = model_memory + forward_pass_memory + gradient_memory + gradient_moment_memory\n",
    "\n",
    "    return total_memory\n",
    "\n",
    "def estimate_memory_inference(model, sample_input, batch_size=1, use_amp=False, device=0):\n",
    "    \"\"\"Predict the maximum memory usage of the model. \n",
    "    Args:\n",
    "        optimizer_type (Type): the class name of the optimizer to instantiate\n",
    "        model (nn.Module): the neural network model\n",
    "        sample_input (torch.Tensor): A sample input to the network. It should be \n",
    "            a single item, not a batch, and it will be replicated batch_size times.\n",
    "        batch_size (int): the batch size\n",
    "        use_amp (bool): whether to estimate based on using mixed precision\n",
    "        device (torch.device): the device to use\n",
    "    \"\"\"\n",
    "    # Reset model and optimizer\n",
    "    model.cpu()\n",
    "    a = torch.cuda.memory_allocated(device)\n",
    "    model.to(device)\n",
    "    b = torch.cuda.memory_allocated(device)\n",
    "    model_memory = b - a\n",
    "    model_input = sample_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "    output = model(model_input.to(device)).sum()\n",
    "    total_memory = model_memory\n",
    "\n",
    "    return total_memory\n",
    "\n",
    "def test_memory_training(in_size=100, out_size=10, hidden_size=100, optimizer_type=torch.optim.Adam, batch_size=1, use_amp=False, device=0):\n",
    "    sample_input = torch.randn(batch_size, in_size, dtype=torch.float32)\n",
    "    model = nn.Sequential(nn.Linear(in_size, hidden_size),\n",
    "                        *[nn.Linear(hidden_size, hidden_size) for _ in range(200)],\n",
    "                        nn.Linear(hidden_size, out_size))\n",
    "    max_mem_est = estimate_memory_training(model, sample_input[0], optimizer_type=optimizer_type, batch_size=batch_size, use_amp=use_amp)\n",
    "    print(\"Maximum Memory Estimate\", max_mem_est)\n",
    "    optimizer = optimizer_type(model.parameters(), lr=.001)\n",
    "    print(\"Beginning mem:\", torch.cuda.memory_allocated(device), \"Note - this may be higher than 0, which is due to PyTorch caching. Don't worry too much about this number\")\n",
    "    model.to(device)\n",
    "    print(\"After model to device:\", torch.cuda.memory_allocated(device))\n",
    "    for i in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        print(\"Iteration\", i)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            a = torch.cuda.memory_allocated(device)\n",
    "            out = model(sample_input.to(device)).sum() # Taking the sum here just to get a scalar output\n",
    "            b = torch.cuda.memory_allocated(device)\n",
    "        print(\"1 - After forward pass\", torch.cuda.memory_allocated(device))\n",
    "        print(\"2 - Memory consumed by forward pass\", b - a)\n",
    "        out.backward()\n",
    "        print(\"3 - After backward pass\", torch.cuda.memory_allocated(device))\n",
    "        optimizer.step()\n",
    "        print(\"4 - After optimizer step\", torch.cuda.memory_allocated(device))\n",
    "\n",
    "def test_memory_inference(in_size=100, out_size=10, hidden_size=100, batch_size=1, use_amp=False, device=0):\n",
    "    sample_input = torch.randn(batch_size, in_size, dtype=torch.float32)\n",
    "    model = nn.Sequential(nn.Linear(in_size, hidden_size),\n",
    "                        *[nn.Linear(hidden_size, hidden_size) for _ in range(200)],\n",
    "                        nn.Linear(hidden_size, out_size))\n",
    "    max_mem_est = estimate_memory_inference(model, sample_input[0], batch_size=batch_size, use_amp=use_amp)\n",
    "    print(\"Maximum Memory Estimate\", max_mem_est)\n",
    "    print(\"Beginning mem:\", torch.cuda.memory_allocated(device), \"Note - this may be higher than 0, which is due to PyTorch caching. Don't worry too much about this number\")\n",
    "    model.to(device)\n",
    "    print(\"After model to device:\", torch.cuda.memory_allocated(device))\n",
    "    with torch.no_grad():\n",
    "        for i in range(3):\n",
    "            print(\"Iteration\", i)\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                a = torch.cuda.memory_allocated(device)\n",
    "                out = model(sample_input.to(device)).sum() # Taking the sum here just to get a scalar output\n",
    "                b = torch.cuda.memory_allocated(device)\n",
    "            print(\"1 - After forward pass\", torch.cuda.memory_allocated(device))\n",
    "            print(\"2 - Memory consumed by forward pass\", b - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_memory_inference(batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
