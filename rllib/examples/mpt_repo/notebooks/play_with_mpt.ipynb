{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.examples.mpt_repo.mpt.module.configuration_mpt import MPTConfig\n",
    "from ray.rllib.examples.mpt_repo.mpt.module.modeling_mpt import MPTForCausalLM, MPTModel, MPTBlock\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.attn_config[\"attn_impl\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPTConfig {\n",
       "  \"attn_config\": {\n",
       "    \"alibi\": false,\n",
       "    \"alibi_bias_max\": 8,\n",
       "    \"attn_impl\": \"torch\",\n",
       "    \"attn_pdrop\": 0.0,\n",
       "    \"attn_type\": \"multihead_attention\",\n",
       "    \"attn_uses_sequence_id\": false,\n",
       "    \"clip_qkv\": null,\n",
       "    \"prefix_lm\": false,\n",
       "    \"qk_ln\": false,\n",
       "    \"softmax_scale\": null\n",
       "  },\n",
       "  \"d_model\": 2048,\n",
       "  \"emb_pdrop\": 0.0,\n",
       "  \"embedding_fraction\": 1.0,\n",
       "  \"expansion_ratio\": 4,\n",
       "  \"init_config\": {\n",
       "    \"fan_mode\": \"fan_in\",\n",
       "    \"init_nonlinearity\": \"relu\",\n",
       "    \"name\": \"kaiming_normal_\"\n",
       "  },\n",
       "  \"init_device\": \"cpu\",\n",
       "  \"learned_pos_emb\": true,\n",
       "  \"logit_scale\": null,\n",
       "  \"max_seq_len\": 2048,\n",
       "  \"model_type\": \"mpt\",\n",
       "  \"n_heads\": 16,\n",
       "  \"n_layers\": 24,\n",
       "  \"no_bias\": false,\n",
       "  \"norm_type\": \"low_precision_layernorm\",\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"transformers_version\": \"4.28.1\",\n",
       "  \"use_cache\": false,\n",
       "  \"verbose\": 0,\n",
       "  \"vocab_size\": 50368\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba789ee7e13d494cab92444e4b029db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8024f05cbd4b0d8f11963a5c1ecaa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa2518b5fad45b78dd05a1f7a39555e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/shared_storage/kourosh/hf_home/modules/transformers_modules/mosaicml/mpt-7b-instruct/bd1748ec173f1c43e11f1973fc6e61cb3de0f327/attention.py:148: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
      "  warnings.warn('Using `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl: triton`.')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf1a96e391e41a88904ed2bc29c1451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131e0b2b76be4eb090937c5bb0bc818e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/91.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = MPTForCausalLM(config)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-7b\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mosaicml/mpt-7b-instruct\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[30003,   310,   271,  9775,   326,  8631,   247,  4836,    15, 19566,\n",
       "           247,  2380,   326, 20420, 29141,   253,  2748,    15,   209,  4118,\n",
       "         41959,    27, 19566,   247,  4288,  5311,   670,   247,   495,    14,\n",
       "          1201,  7408,   281, 23174,    15,   209,  4118, 19371,    27,   209]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([\"Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Write a travel blog about a 3-day trip to Thailand. ### Response: \"], return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30003,   310,   271,  9775,   326,  8631,   247,  4836,    15, 19566,\n",
       "           247,  2380,   326, 20420, 29141,   253,  2748,    15,   209,  4118,\n",
       "         41959,    27, 19566,   247,  4288,  5311,   670,   247,   495,    14,\n",
       "          1201,  7408,   281, 23174,    15,   209,  4118, 19371,    27,   209,\n",
       "           187, 15490,   337,    27, 48771,   187,  2906, 30487,   275, 48771]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=10\n",
    ")\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Write a travel blog about a 3-day trip to Thailand. ### Response: \\nDay 1: Bangkok\\nArrived in Bangkok'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text = tokenizer.decode(outputs[0])\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
