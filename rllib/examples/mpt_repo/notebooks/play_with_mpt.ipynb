{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpt.module.configuration_mpt import MPTConfig\n",
    "from mpt.module.modeling_mpt import MPTForCausalLM, MPTModel, MPTBlock\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.attn_config[\"attn_impl\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPTConfig {\n",
       "  \"attn_config\": {\n",
       "    \"alibi\": false,\n",
       "    \"alibi_bias_max\": 8,\n",
       "    \"attn_impl\": \"torch\",\n",
       "    \"attn_pdrop\": 0.0,\n",
       "    \"attn_type\": \"multihead_attention\",\n",
       "    \"attn_uses_sequence_id\": false,\n",
       "    \"clip_qkv\": null,\n",
       "    \"prefix_lm\": false,\n",
       "    \"qk_ln\": false,\n",
       "    \"softmax_scale\": null\n",
       "  },\n",
       "  \"d_model\": 2048,\n",
       "  \"emb_pdrop\": 0.0,\n",
       "  \"embedding_fraction\": 1.0,\n",
       "  \"expansion_ratio\": 4,\n",
       "  \"init_config\": {\n",
       "    \"fan_mode\": \"fan_in\",\n",
       "    \"init_nonlinearity\": \"relu\",\n",
       "    \"name\": \"kaiming_normal_\"\n",
       "  },\n",
       "  \"init_device\": \"cpu\",\n",
       "  \"learned_pos_emb\": true,\n",
       "  \"logit_scale\": null,\n",
       "  \"max_seq_len\": 2048,\n",
       "  \"model_type\": \"mpt\",\n",
       "  \"n_heads\": 16,\n",
       "  \"n_layers\": 24,\n",
       "  \"no_bias\": false,\n",
       "  \"norm_type\": \"low_precision_layernorm\",\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"transformers_version\": \"4.28.1\",\n",
       "  \"use_cache\": false,\n",
       "  \"verbose\": 0,\n",
       "  \"vocab_size\": 50368\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/default/mpt_repo/mpt/module/attention.py:148: UserWarning: Using `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm` we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n",
      "  warnings.warn('Using `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl: triton`.')\n"
     ]
    }
   ],
   "source": [
    "model = MPTForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairscale.nn.checkpoint import checkpoint_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = checkpoint_wrapper(model, offload_to_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[25521,   619,  1416,   310,   465,   276,   287,  1200,    32,   209]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([\"hello my name is kourosh? \"], return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1252,  1.4075, -1.8487,  ..., -0.5698,  1.6197,  1.9951],\n",
      "         [ 0.3545,  1.5721, -1.5741,  ..., -0.7453,  1.2746,  1.9712],\n",
      "         [ 1.0514,  1.0456, -2.1576,  ..., -0.6029,  1.8335,  2.0054],\n",
      "         ...,\n",
      "         [ 0.8154,  0.5779, -0.8116,  ..., -0.6054,  1.6809,  2.6405],\n",
      "         [ 0.2411,  0.5816, -1.6374,  ..., -1.1922,  1.7143,  2.7007],\n",
      "         [ 1.0074,  0.9751, -1.9049,  ..., -1.4873,  1.9630,  2.6677]]])\n"
     ]
    }
   ],
   "source": [
    "output = model.forward(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
