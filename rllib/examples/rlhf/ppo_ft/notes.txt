
04/10


04/09

- I created a RLHF sampler that I can use to collect samples, but it only works for one sample at a time right now. I need to extend it to produce batches of experiences. 

- In order to share the module between learner and Sampler I assumed the learner is running in local mode, but we need to generalize this requirement. 

- I created a simple Buffer that collects all experiences and can convert them to sample batches, with the correct padding and everything. 

04/07

- I realized the RLlib API does not allow us to initialize our models easily. This should be super intuitive to do. This feature is absolutely necessary for any fine-tuning or RL on top of a pre-trained network. 

- For PPO-ptx, learner needs to be customized, we should make it super easy, especially when we have custom kwargs.

- There is another complication regarding running multiple parallel envs on separate machines. Each env owns a few LLMs (SFT and RM). Each env replica should independently download their own weights and load them up. The optimization we can do here is that these models are only needed for inference, so we can do inference optimization to make them faster / smaller (e.g. 4 bit quantization, etc.)

- I realized our implementation of PPORLModule is not fully extendible. This is a bad implementation on PPORLModule not the base classes. --> I fixed the base classes so that it becomes super easy to by-pass the catalog logic. Catalog should remained an RLLib specific thing so that we can programatically cover various action/obs spaces, while still allowing for spaces we haven't covered. 

- _initialize_loss_from_dummy_batch() is a stupid logic that we should get rid off experimentally. I have added an experimental flag to disable it upon request. We should later get rid of this thing.

- RLlib's sampler is so limiting. In this env I need an action space that is of Repeated nature. The only supported action spaces are implicitly Box, Discrete, and MultiDiscrete. The error message also doesn't help much, since it gets raised very late in the agent_collector's .get_dummy_values() method. agent_collector wants to run this on actions and it fails to produce dummy values. 

04/06

- I got the RLHF env working. 
- The tricky part is getting observation / action spaces to correspond to what is expected. There is not much value in this except that when using RLlib obs and action space are the things that gets passed to the model constructor (which is ignored in case of LLMs)
- Another tricky that required some thinking was formulating the env as bandit in LLMs.
You want the env to read the dataset and accept a completion and then call the episode done. This type of env can be massively batched and should run on gpus, and we should not necessarily adhere to RLlib's assumption on sequential decision making envs, and also keeping things in numpy land. 

The complication is that I have to go back and forth between numpy to torch tensors and back. 



